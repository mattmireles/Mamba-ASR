# ############################################################################
# LIBRISPEECH DATA PREPARATION PIPELINE - Mamba-ASR Dataset Processing
# ############################################################################
#
# SYSTEM ROLE:
# ============
# This script serves as the primary data preparation pipeline for LibriSpeech
# dataset integration with the Mamba-ASR training system. It transforms raw
# LibriSpeech FLAC audio files and transcription text into structured CSV
# manifests required by SpeechBrain's DataIO framework.
#
# ARCHITECTURAL INTEGRATION:
# ==========================
# The data preparation pipeline integrates with the Mamba-ASR system through:
# - train_CTC.py: Loads CSV manifests generated by this script for CTC training
# - train_S2S.py: Uses prepared datasets for sequence-to-sequence training
# - hparams/*.yaml: References CSV paths created by this preparation pipeline
# - SpeechBrain DataIO: Consumes CSV format for efficient batching and loading
#
# KEY PROCESSING STAGES:
# ======================
# 1. Dataset Validation: Verifies LibriSpeech folder structure and split availability
# 2. Audio Indexing: Discovers and catalogs all FLAC audio files across splits
# 3. Transcript Processing: Parses transcription files and maps to audio utterances
# 4. Metadata Extraction: Reads audio duration, speaker IDs, and file paths
# 5. CSV Generation: Creates structured manifests with audio-text alignments
# 6. Optional Extensions: Lexicon creation for G2P modeling and dataset merging
#
# PERFORMANCE OPTIMIZATIONS:
# ==========================
# - Parallel Processing: Uses SpeechBrain's parallel_map for concurrent audio metadata reading
# - Chunked Processing: Optimizes I/O with configurable chunk sizes for FLAC reading
# - Skip Logic: Avoids reprocessing when CSV files already exist and configuration unchanged
# - Memory Efficiency: Streams processing without loading entire dataset into memory
#
# DATA FLOW ARCHITECTURE:
# =======================
# LibriSpeech Raw Data → Audio Discovery → Transcript Parsing → Metadata Extraction 
#        ↓
# CSV Manifest Generation → Optional Lexicon Creation → Training Pipeline Integration
#        ↓
# train_CTC.py / train_S2S.py → SpeechBrain DataLoader → Mamba-ASR Training
#
# INTEGRATION WITH TRAINING:
# ==========================
# This script must be executed before any training can begin. The generated CSV
# files serve as the bridge between raw LibriSpeech data and the training scripts:
# - Generated CSV paths are referenced in hparams/*.yaml configurations
# - Training scripts load these CSVs through SpeechBrain's make_dataloader()
# - Audio file paths in CSVs must remain valid throughout training process
#
# Authors: Mirco Ravanelli, Ju-Chieh Chou, Loren Lugosch, Pierre Champion, Adel Moumen
# Download: http://www.openslr.org/12
# ############################################################################

"""
LibriSpeech Data Preparation Pipeline for Mamba-ASR Training System.

This module provides comprehensive data preparation functionality for converting
raw LibriSpeech dataset into structured CSV manifests required by the Mamba-ASR
training pipeline. Integrates with train_CTC.py and train_S2S.py through
SpeechBrain's DataIO framework.

Download: http://www.openslr.org/12

Authors:
 * Mirco Ravanelli, 2020
 * Ju-Chieh Chou, 2020  
 * Loren Lugosch, 2020
 * Pierre Champion, 2023
 * Adel Moumen, 2024
"""

import csv
import functools
import logging
import os
import random
from collections import Counter
from dataclasses import dataclass

from speechbrain.dataio.dataio import (
    load_pkl,
    merge_csvs,
    read_audio_info,
    save_pkl,
)
from speechbrain.utils.data_utils import download_file, get_all_files
from speechbrain.utils.parallel import parallel_map

logger = logging.getLogger(__name__)

# =============================================================================
# DATA PREPARATION CONSTANTS - LibriSpeech Processing Configuration
# =============================================================================

class DataPreparationConstants:
    """Named constants for LibriSpeech data preparation pipeline.
    
    These constants ensure consistent configuration across data preparation
    stages and provide clear documentation for AI developers modifying
    the preprocessing pipeline.
    """
    
    # Configuration persistence for skip logic and reproducibility
    OPT_FILE = "opt_librispeech_prepare.pkl"  # Stores preparation options for skip detection
    
    # Audio processing specifications aligned with LibriSpeech standard
    SAMPLE_RATE = 16000  # Standard LibriSpeech sampling rate (16kHz)
    # 16kHz provides sufficient bandwidth for speech (human speech: 80Hz-8kHz)
    # All LibriSpeech audio resampled to this rate for consistency
    
    # External resource URLs for language model and lexicon download
    OPEN_SLR_11_BASE_URL = "http://www.openslr.org/resources/11/"
    # OpenSLR repository containing LibriSpeech auxiliary resources
    
    LEXICON_DOWNLOAD_URL = "http://www.openslr.org/resources/11/librispeech-lexicon.txt"
    # Pronunciation dictionary for grapheme-to-phoneme modeling
    
    # N-gram language models available for download and rescoring
    AVAILABLE_NGRAM_MODELS = [
        "3-gram.arpa.gz",                # Standard 3-gram model for decoding
        "3-gram.pruned.1e-7.arpa.gz",   # Pruned 3-gram for memory efficiency  
        "3-gram.pruned.3e-7.arpa.gz",   # More aggressively pruned 3-gram
        "4-gram.arpa.gz",               # Large 4-gram for maximum accuracy
    ]
    
    # Parallel processing optimization constants
    FLAC_PROCESSING_CHUNK_SIZE = 8192  # Optimal chunk size for FLAC metadata reading
    # Large chunks reduce main thread CPU bottlenecks in parallel processing
    
    # Default lexicon split ratios for G2P model training
    DEFAULT_LEXICON_SPLIT_RATIO = [98, 1, 1]  # Train: 98%, Valid: 1%, Test: 1%
    
    # CSV output format configuration
    CSV_HEADERS = ["ID", "duration", "wav", "spk_id", "wrd"]  # Standard SpeechBrain format
    LEXICON_CSV_HEADERS = "ID,duration,char,phn\n"  # G2P CSV format

# Legacy constants for backward compatibility - use DataPreparationConstants instead
OPT_FILE = DataPreparationConstants.OPT_FILE
SAMPLERATE = DataPreparationConstants.SAMPLE_RATE
OPEN_SLR_11_LINK = DataPreparationConstants.OPEN_SLR_11_BASE_URL
OPEN_SLR_11_NGRAM_MODELs = DataPreparationConstants.AVAILABLE_NGRAM_MODELS


def prepare_librispeech(
    data_folder,
    save_folder,
    tr_splits=[],
    dev_splits=[],
    te_splits=[],
    select_n_sentences=None,
    merge_lst=[],
    merge_name=None,
    create_lexicon=False,
    skip_prep=False,
):
    """
    This class prepares the csv files for the LibriSpeech dataset.
    Download link: http://www.openslr.org/12

    Arguments
    ---------
    data_folder : str
        Path to the folder where the original LibriSpeech dataset is stored.
    save_folder : str
        The directory where to store the csv files.
    tr_splits : list
        List of train splits to prepare from ['test-others','train-clean-100',
        'train-clean-360','train-other-500'].
    dev_splits : list
        List of dev splits to prepare from ['dev-clean','dev-others'].
    te_splits : list
        List of test splits to prepare from ['test-clean','test-others'].
    select_n_sentences : int
        Default : None
        If not None, only pick this many sentences.
    merge_lst : list
        List of librispeech splits (e.g, train-clean, train-clean-360,..) to
        merge in a single csv file.
    merge_name: str
        Name of the merged csv file.
    create_lexicon: bool
        If True, it outputs csv files containing mapping between grapheme
        to phonemes. Use it for training a G2P system.
    skip_prep: bool
        If True, data preparation is skipped.

    Returns
    -------
    None

    Example
    -------
    >>> data_folder = 'datasets/LibriSpeech'
    >>> tr_splits = ['train-clean-100']
    >>> dev_splits = ['dev-clean']
    >>> te_splits = ['test-clean']
    >>> save_folder = 'librispeech_prepared'
    >>> prepare_librispeech(data_folder, save_folder, tr_splits, dev_splits, te_splits)
    """

    if skip_prep:
        return
    data_folder = data_folder
    splits = tr_splits + dev_splits + te_splits
    save_folder = save_folder
    select_n_sentences = select_n_sentences
    conf = {
        "select_n_sentences": select_n_sentences,
    }

    # Other variables
    # Saving folder
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)

    save_opt = os.path.join(save_folder, OPT_FILE)

    # Check if this phase is already done (if so, skip it)
    if skip(splits, save_folder, conf):
        logger.info("Skipping preparation, completed in previous run.")
        return
    else:
        logger.info("Data_preparation...")

    # Additional checks to make sure the data folder contains Librispeech
    check_librispeech_folders(data_folder, splits)

    # create csv files for each split
    all_texts = {}
    for split_index in range(len(splits)):
        split = splits[split_index]

        wav_lst = get_all_files(
            os.path.join(data_folder, split), match_and=[".flac"]
        )

        text_lst = get_all_files(
            os.path.join(data_folder, split), match_and=["trans.txt"]
        )

        text_dict = text_to_dict(text_lst)
        all_texts.update(text_dict)

        if select_n_sentences is not None:
            n_sentences = select_n_sentences[split_index]
        else:
            n_sentences = len(wav_lst)

        create_csv(save_folder, wav_lst, text_dict, split, n_sentences)

    # Merging csv file if needed
    if merge_lst and merge_name is not None:
        merge_files = [split_libri + ".csv" for split_libri in merge_lst]
        merge_csvs(
            data_folder=save_folder, csv_lst=merge_files, merged_csv=merge_name
        )

    # Create lexicon.csv and oov.csv
    if create_lexicon:
        create_lexicon_and_oov_csv(all_texts, save_folder)

    # saving options
    save_pkl(conf, save_opt)


def create_lexicon_and_oov_csv(all_texts, save_folder):
    """Creates pronunciation lexicon CSV files for grapheme-to-phoneme (G2P) model training.
    
    This function generates lexicon resources by downloading the official LibriSpeech
    pronunciation dictionary and converting it into CSV format suitable for G2P
    model training. Essential for applications requiring pronunciation modeling
    or phonetic analysis.
    
    G2P MODEL INTEGRATION:
    =====================
    Generated files support:
    - G2P model training: Maps graphemes (letters) to phonemes (sounds)
    - Pronunciation analysis: Enables phonetic transcription of text
    - Out-of-vocabulary handling: Identifies words missing from lexicon
    - Speech synthesis: Provides pronunciation for TTS systems
    
    PROCESSING WORKFLOW:
    ===================
    1. Lexicon Download: Retrieves official LibriSpeech pronunciation dictionary
    2. Vocabulary Analysis: Extracts all words from transcription text
    3. Lexicon Parsing: Processes pronunciation entries with phoneme cleanup
    4. CSV Generation: Creates structured G2P training data
    5. Dataset Splitting: Divides lexicon into train/validation/test sets
    
    LEXICON DATA FORMAT:
    ===================
    Input LibriSpeech lexicon format:
    WORD P H O N E M E S
    
    Output CSV format:
    ID,duration,char,phn
    0,5,"A B O U T","AH B AW T"
    
    Generated Files:
    - lexicon.csv: Complete pronunciation dictionary in CSV format
    - lexicon_tr.csv: Training split (98% of entries)
    - lexicon_dev.csv: Validation split (1% of entries)  
    - lexicon_test.csv: Test split (1% of entries)
    
    Arguments
    ---------
    all_texts : dict
        Dictionary containing transcription text from all LibriSpeech splits.
        Used to identify vocabulary coverage and potential OOV words
        
    save_folder : str
        Directory where lexicon CSV files will be stored.
        Must be writable for CSV generation and lexicon download
        
    Side Effects
    ------------
    - Downloads librispeech-lexicon.txt if not already present
    - Creates lexicon.csv with complete pronunciation dictionary
    - Generates train/dev/test splits for G2P model training
    - Logs lexicon processing progress and statistics
    
    Notes
    -----
    The function strips stress markers (0-9) from phonemes to create
    canonical pronunciation representations suitable for most G2P
    applications. Stress information can be preserved by modifying
    the phoneme processing logic if required.
    """
    # If the lexicon file does not exist, download it
    lexicon_url = DataPreparationConstants.LEXICON_DOWNLOAD_URL
    lexicon_path = os.path.join(save_folder, "librispeech-lexicon.txt")

    if not os.path.isfile(lexicon_path):
        logger.info(
            "Lexicon file not found. Downloading from %s." % lexicon_url
        )
        download_file(lexicon_url, lexicon_path)

    # Get list of all words in the transcripts
    transcript_words = Counter()
    for key in all_texts:
        transcript_words.update(all_texts[key].split("_"))

    # Get list of all words in the lexicon
    lexicon_words = []
    lexicon_pronunciations = []
    with open(lexicon_path, "r") as f:
        lines = f.readlines()
        for line in lines:
            word = line.split()[0]
            pronunciation = line.split()[1:]
            lexicon_words.append(word)
            lexicon_pronunciations.append(pronunciation)

    # Create lexicon.csv
    header = DataPreparationConstants.LEXICON_CSV_HEADERS
    lexicon_csv_path = os.path.join(save_folder, "lexicon.csv")
    with open(lexicon_csv_path, "w") as f:
        f.write(header)
        for idx in range(len(lexicon_words)):
            separated_graphemes = [c for c in lexicon_words[idx]]
            duration = len(separated_graphemes)
            graphemes = " ".join(separated_graphemes)
            pronunciation_no_numbers = [
                p.strip("0123456789") for p in lexicon_pronunciations[idx]
            ]
            phonemes = " ".join(pronunciation_no_numbers)
            line = (
                ",".join([str(idx), str(duration), graphemes, phonemes]) + "\n"
            )
            f.write(line)
    logger.info("Lexicon written to %s." % lexicon_csv_path)

    # Split lexicon.csv in train, validation, and test splits
    split_lexicon(save_folder, DataPreparationConstants.DEFAULT_LEXICON_SPLIT_RATIO)


def split_lexicon(data_folder, split_ratio):
    """Splits pronunciation lexicon into train/validation/test sets for G2P model training.
    
    This function partitions the complete pronunciation lexicon into separate
    datasets required for systematic G2P model development. Ensures balanced
    vocabulary distribution across training phases.
    
    MACHINE LEARNING DATASET CREATION:
    ==================================
    Follows standard ML practices:
    - Random shuffling: Prevents alphabetical or frequency bias
    - Balanced splitting: Maintains vocabulary diversity across splits
    - Reproducible partitioning: Consistent splits for fair model comparison
    
    SPLIT STRATEGY:
    ==============
    Default ratio (98:1:1) optimized for lexicon characteristics:
    - Large training set: Maximizes G2P pattern learning
    - Small validation set: Sufficient for hyperparameter tuning
    - Small test set: Reliable final evaluation on unseen words
    
    INTEGRATION WITH G2P TRAINING:
    ==============================
    Generated splits enable:
    - Systematic model development with proper train/dev/test separation
    - Hyperparameter optimization on validation set
    - Unbiased final evaluation on test set
    - Consistent comparison across different G2P architectures
    
    Arguments
    ---------
    data_folder : str
        Path to directory containing lexicon.csv file to split.
        Output files will be created in the same directory
        
    split_ratio : list
        Three-element list with [train_percent, valid_percent, test_percent].
        Must sum to 100. Default: [98, 1, 1] for lexicon data
        
    Generated Files
    ---------------
    - lexicon_tr.csv: Training split with specified percentage of entries
    - lexicon_dev.csv: Validation split for hyperparameter tuning
    - lexicon_test.csv: Test split for final model evaluation
    
    Notes
    -----
    The function preserves the CSV header in all output files and uses
    random shuffling to ensure unbiased vocabulary distribution across
    splits. The small validation/test sets are appropriate for lexicon
    data where training requires maximum vocabulary coverage.
    """
    # Reading lexicon.csv
    lexicon_csv_path = os.path.join(data_folder, "lexicon.csv")
    with open(lexicon_csv_path, "r") as f:
        lexicon_lines = f.readlines()
    # Remove header
    lexicon_lines = lexicon_lines[1:]

    # Shuffle entries
    random.shuffle(lexicon_lines)

    # Selecting lines
    header = DataPreparationConstants.LEXICON_CSV_HEADERS

    tr_snts = int(0.01 * split_ratio[0] * len(lexicon_lines))
    train_lines = [header] + lexicon_lines[0:tr_snts]
    valid_snts = int(0.01 * split_ratio[1] * len(lexicon_lines))
    valid_lines = [header] + lexicon_lines[tr_snts : tr_snts + valid_snts]
    test_lines = [header] + lexicon_lines[tr_snts + valid_snts :]

    # Saving files
    with open(os.path.join(data_folder, "lexicon_tr.csv"), "w") as f:
        f.writelines(train_lines)
    with open(os.path.join(data_folder, "lexicon_dev.csv"), "w") as f:
        f.writelines(valid_lines)
    with open(os.path.join(data_folder, "lexicon_test.csv"), "w") as f:
        f.writelines(test_lines)


@dataclass
class LSRow:
    """Data structure representing a single LibriSpeech utterance for CSV generation.
    
    This dataclass encapsulates all metadata required for a single row in the
    generated CSV manifest. Used by parallel processing pipeline to structure
    audio metadata extraction results.
    
    USAGE IN PROCESSING PIPELINE:
    =============================
    - process_line(): Returns LSRow instance with extracted metadata
    - create_csv(): Consumes LSRow objects to generate CSV rows
    - Parallel processing: Enables efficient data transfer between processes
    
    Attributes
    ----------
    snt_id : str
        Unique sentence/utterance identifier from LibriSpeech.
        Format: speaker-chapter-utterance (e.g., '1089-134686-0000')
        
    spk_id : str
        Speaker identifier extracted from sentence ID.
        Format: speaker-chapter (e.g., '1089-134686')
        
    duration : float
        Audio duration in seconds extracted from FLAC metadata.
        Required by SpeechBrain for batch processing and memory estimation
        
    file_path : str
        Absolute path to the FLAC audio file.
        Must remain valid throughout training process
        
    words : str
        Transcription text with spaces (converted from underscore format).
        Ready for direct use in training pipeline
    """
    snt_id: str
    spk_id: str
    duration: float
    file_path: str
    words: str


def process_line(wav_file, text_dict) -> LSRow:
    """Extracts metadata from a single FLAC audio file for CSV generation.
    
    This function processes one LibriSpeech audio file and extracts all metadata
    required for training pipeline integration. Designed for parallel processing
    to optimize I/O operations across large datasets.
    
    INTEGRATION WITH PARALLEL PROCESSING:
    ====================================
    Called by:
    - create_csv(): Via parallel_map with functools.partial closure
    - Parallel workers: Each worker processes subset of audio files
    
    Uses:
    - SpeechBrain read_audio_info(): Efficient FLAC metadata extraction
    - LibriSpeech naming convention: Extracts IDs from standardized filenames
    
    METADATA EXTRACTION PROCESS:
    ============================
    1. Filename Parsing: Extracts utterance ID from FLAC filename
    2. Speaker ID Derivation: Creates speaker ID from utterance components
    3. Transcript Lookup: Retrieves text from preprocessed text dictionary
    4. Audio Analysis: Reads FLAC metadata for duration calculation
    5. Text Formatting: Converts underscores to spaces for readability
    
    Arguments
    ---------
    wav_file : str
        Absolute path to LibriSpeech FLAC audio file.
        Must follow LibriSpeech naming convention: speaker-chapter-utterance.flac
        
    text_dict : dict
        Dictionary mapping utterance IDs to transcription text.
        Created by text_to_dict() function from LibriSpeech transcription files
        
    Returns
    -------
    LSRow
        Structured data containing all metadata for CSV row generation.
        Ready for direct conversion to CSV format by create_csv()
        
    Notes
    -----
    This function is designed for parallel execution and must be stateless
    except for the text_dict dependency. The use of functools.partial in
    create_csv() enables efficient parallel processing while maintaining
    access to transcription data.
    """
    snt_id = wav_file.split("/")[-1].replace(".flac", "")
    spk_id = "-".join(snt_id.split("-")[0:2])
    wrds = text_dict[snt_id]
    wrds = " ".join(wrds.split("_"))

    info = read_audio_info(wav_file)
    duration = info.num_frames / info.sample_rate

    return LSRow(
        snt_id=snt_id,
        spk_id=spk_id,
        duration=duration,
        file_path=wav_file,
        words=wrds,
    )


def create_csv(save_folder, wav_lst, text_dict, split, select_n_sentences):
    """Creates structured CSV manifest for a specific LibriSpeech split.
    
    This function processes a single LibriSpeech data split (e.g., train-clean-100)
    and generates a CSV manifest compatible with SpeechBrain's DataIO framework.
    The CSV contains audio file paths, durations, speaker IDs, and transcriptions
    required for training.
    
    INTEGRATION WITH TRAINING PIPELINE:
    ===================================
    Called by:
    - prepare_librispeech(): Main orchestration function for each data split
    - Parallel processing: Each split processed independently for efficiency
    
    Generated CSV consumed by:
    - train_CTC.py: Loads via SpeechBrain make_dataloader() for CTC training
    - train_S2S.py: Uses for sequence-to-sequence model training
    - hparams/*.yaml: References generated CSV paths in configuration
    
    PERFORMANCE OPTIMIZATIONS:
    ==========================
    - Parallel Audio Processing: Uses SpeechBrain's parallel_map with optimized chunking
    - Skip Logic: Avoids regenerating CSV if file already exists
    - Memory Efficiency: Streams processing without loading entire split into memory
    - FLAC Optimization: Large chunk sizes reduce main thread CPU bottlenecks
    
    CSV FORMAT AND STRUCTURE:
    =========================
    Generated CSV follows SpeechBrain standard format:
    - ID: Unique utterance identifier (LibriSpeech format: speaker-chapter-utterance)
    - duration: Audio length in seconds (extracted from FLAC metadata)
    - wav: Absolute path to FLAC audio file
    - spk_id: Speaker identifier (first two components of utterance ID)
    - wrd: Transcript text with underscores replaced by spaces
    
    Arguments
    ---------
    save_folder : str
        Location of the folder for storing the generated CSV file.
        CSV named as '{split}.csv' (e.g., train-clean-100.csv)
        
    wav_lst : list
        The list of FLAC audio file paths for the current data split.
        Discovered by get_all_files() with .flac extension matching
        
    text_dict : dict
        Dictionary mapping utterance IDs to transcription text.
        Created by text_to_dict() from LibriSpeech transcription files
        
    split : str
        The name of the current data split being processed.
        Used for CSV filename and logging identification
        
    select_n_sentences : int, optional
        Maximum number of sentences to include in CSV.
        If None, processes all available audio files.
        Useful for creating smaller datasets or debugging

    Returns
    -------
    None
        Function modifies filesystem by creating CSV file in save_folder.
        CSV path: {save_folder}/{split}.csv
        
    Side Effects
    ------------
    - Creates CSV file with SpeechBrain-compatible format
    - Logs processing progress and completion status
    - Skips processing if CSV already exists (efficiency optimization)
    
    Notes
    -----
    The parallel processing uses functools.partial to create a closure
    with text_dict, enabling efficient parallel metadata extraction
    while maintaining access to transcription data.
    
    Example Generated CSV
    ---------------------
    ID,duration,wav,spk_id,wrd
    1089-134686-0000,5.535,/data/train-clean-100/1089/134686/1089-134686-0000.flac,1089-134686,HE BEGAN A CONFUSED COMPLAINT AGAINST THE WIZARD
    """
    # Setting path for the csv file
    csv_file = os.path.join(save_folder, split + ".csv")
    if os.path.exists(csv_file):
        logger.info("Csv file %s already exists, not recreating." % csv_file)
        return

    # Preliminary prints
    msg = "Creating csv lists in  %s..." % (csv_file)
    logger.info(msg)

    csv_lines = [DataPreparationConstants.CSV_HEADERS]

    snt_cnt = 0
    line_processor = functools.partial(process_line, text_dict=text_dict)
    # Processing all the wav files in wav_lst
    # FLAC metadata reading is already fast, so we set a high chunk size
    # to limit main thread CPU bottlenecks
    for row in parallel_map(line_processor, wav_lst, chunk_size=DataPreparationConstants.FLAC_PROCESSING_CHUNK_SIZE):
        csv_line = [
            row.snt_id,
            str(row.duration),
            row.file_path,
            row.spk_id,
            row.words,
        ]

        # Appending current file to the csv_lines list
        csv_lines.append(csv_line)

        snt_cnt = snt_cnt + 1

        # parallel_map guarantees element ordering so we're OK
        if snt_cnt == select_n_sentences:
            break

    # Writing the csv_lines
    with open(csv_file, mode="w") as csv_f:
        csv_writer = csv.writer(
            csv_f, delimiter=",", quotechar='"', quoting=csv.QUOTE_MINIMAL
        )

        for line in csv_lines:
            csv_writer.writerow(line)

    # Final print
    msg = "%s successfully created!" % (csv_file)
    logger.info(msg)


def skip(splits, save_folder, conf):
    """Determines whether data preparation can be skipped based on existing files and configuration.
    
    This function implements intelligent skip logic to avoid reprocessing LibriSpeech
    data when CSV files already exist and configuration hasn't changed. Critical
    for efficient development workflows and resuming interrupted processing.
    
    SKIP DETECTION LOGIC:
    ====================
    1. CSV File Existence: Checks if all required split CSV files exist
    2. Configuration Consistency: Compares current config with saved options
    3. File Integrity: Ensures all expected outputs are present and valid
    
    INTEGRATION WITH WORKFLOW:
    =========================
    Called by:
    - prepare_librispeech(): Early exit optimization to avoid reprocessing
    - Development workflows: Enables rapid iteration without data regeneration
    
    Benefits:
    - Time Savings: Avoids hours of reprocessing for large datasets
    - Consistency: Ensures same configuration produces same outputs
    - Development Efficiency: Allows quick testing of training modifications
    
    CONFIGURATION PERSISTENCE:
    ==========================
    Uses DataPreparationConstants.OPT_FILE (opt_librispeech_prepare.pkl) to store:
    - select_n_sentences parameter for reproducibility
    - Other configuration options that affect output
    - Enables detection of configuration changes requiring reprocessing
    
    Arguments
    ---------
    splits : list
        List of data splits expected in the preparation.
        Function checks for corresponding CSV files (e.g., 'train-clean-100.csv')
        
    save_folder : str
        Directory where CSV files and configuration should be stored.
        Must exist and be writable for configuration persistence
        
    conf : dict
        Current configuration options for comparison with saved settings.
        Typically contains select_n_sentences and other processing parameters
        
    Returns
    -------
    bool
        True if preparation can be skipped (all files exist, config unchanged).
        False if preparation must be performed (missing files or config change).
        
    Notes
    -----
    The function uses a conservative approach - any missing file or configuration
    change triggers reprocessing to ensure data integrity. This prevents subtle
    bugs from inconsistent data preparation.
    """

    # Checking csv files
    skip = True

    for split in splits:
        if not os.path.isfile(os.path.join(save_folder, split + ".csv")):
            skip = False

    #  Checking saved options
    save_opt = os.path.join(save_folder, OPT_FILE)
    if skip is True:
        if os.path.isfile(save_opt):
            opts_old = load_pkl(save_opt)
            if opts_old == conf:
                skip = True
            else:
                skip = False
        else:
            skip = False

    return skip


def text_to_dict(text_lst):
    """Converts LibriSpeech transcription files into utterance-to-text dictionary.
    
    This function processes all transcription files for a LibriSpeech split and
    creates a unified dictionary mapping utterance IDs to their corresponding
    transcription text. Essential for linking audio files to their ground truth.
    
    LIBRISPEECH TRANSCRIPTION FORMAT:
    ================================= 
    LibriSpeech transcription files (*.trans.txt) contain lines with format:
    utterance_id TRANSCRIPTION TEXT WITH SPACES
    
    Example:
    1089-134686-0000 HE BEGAN A CONFUSED COMPLAINT AGAINST THE WIZARD
    1089-134686-0001 WHO HAD VANISHED BEHIND THE TREE
    
    INTEGRATION WITH PROCESSING PIPELINE:
    ====================================
    Called by:
    - prepare_librispeech(): Processes transcription files for each split
    - Used before create_csv(): Provides transcript lookup for audio processing
    
    Used by:
    - process_line(): Looks up transcriptions during parallel audio processing
    - create_csv(): Requires text_dict for complete metadata extraction
    
    TEXT PREPROCESSING:
    ==================
    - Splits each line on whitespace to separate ID from transcript
    - Joins transcript words with underscores (LibriSpeech convention)
    - Preserves original text casing and punctuation
    - Handles multiple transcription files per split automatically
    
    Arguments
    ---------
    text_lst : list
        List of paths to LibriSpeech transcription files (*.trans.txt).
        Discovered by get_all_files() with 'trans.txt' matching pattern
        
    Returns
    -------
    dict
        Dictionary mapping utterance IDs (str) to transcription text (str).
        Keys: LibriSpeech utterance IDs (e.g., '1089-134686-0000')
        Values: Transcription text with underscores (e.g., 'HE_BEGAN_A_CONFUSED')
        
    Notes
    -----
    The function accumulates transcriptions from multiple files, enabling
    processing of splits that contain multiple speakers/chapters with
    separate transcription files.
    
    The underscore format is converted back to spaces during CSV generation
    in the process_line() function for final training compatibility.
    """
    # Initialization of the text dictionary
    text_dict = {}
    # Reading all the transcription files is text_lst
    for file in text_lst:
        with open(file, "r") as f:
            # Reading all line of the transcription file
            for line in f:
                line_lst = line.strip().split(" ")
                text_dict[line_lst[0]] = "_".join(line_lst[1:])
    return text_dict


def check_librispeech_folders(data_folder, splits):
    """Validates LibriSpeech dataset structure before processing begins.
    
    This function performs essential validation of the LibriSpeech dataset
    structure to ensure all required splits exist before beginning the
    time-intensive data preparation process. Prevents wasted processing
    time on incomplete or incorrectly structured datasets.
    
    LIBRISPEECH EXPECTED STRUCTURE:
    ===============================
    data_folder/
    ├── train-clean-100/
    │   ├── 1089/
    │   │   └── 134686/
    │   │       ├── 1089-134686.trans.txt
    │   │       └── *.flac
    ├── train-clean-360/
    ├── train-other-500/
    ├── dev-clean/
    ├── dev-other/
    ├── test-clean/
    └── test-other/
    
    INTEGRATION WITH ERROR HANDLING:
    ================================
    Called by:
    - prepare_librispeech(): Early validation before processing begins
    - Error prevention: Catches structural issues before expensive operations
    
    Prevents:
    - Hours of processing time wasted on incomplete datasets
    - Cryptic errors deep in the processing pipeline
    - Silent failures that produce partial results
    
    VALIDATION STRATEGY:
    ===================
    - Existence Check: Verifies each split directory exists
    - Early Failure: Raises descriptive error immediately on first missing split
    - Clear Messaging: Provides specific path information for debugging
    
    Arguments
    ---------
    data_folder : str
        Root path to LibriSpeech dataset directory.
        Should contain subdirectories for each data split
        
    splits : list
        List of LibriSpeech split names to validate.
        Each split should correspond to a subdirectory in data_folder
        
    Raises
    ------
    OSError
        If any required LibriSpeech split directory is missing.
        Error message includes specific missing path for troubleshooting
        
    Notes
    -----
    This function only validates directory existence, not content integrity.
    Missing FLAC files or transcription files will be caught later in the
    processing pipeline with appropriate error messages.
    """
    # Checking if all the splits exist
    for split in splits:
        split_folder = os.path.join(data_folder, split)
        if not os.path.exists(split_folder):
            err_msg = (
                "the folder %s does not exist (it is expected in the "
                "Librispeech dataset)" % split_folder
            )
            raise OSError(err_msg)


def download_librispeech_vocab_text(destination):
    """Downloads LibriSpeech vocabulary file for language model training.
    
    This utility function retrieves the official LibriSpeech vocabulary
    from OpenSLR repository. The vocabulary file contains word frequency
    statistics useful for language model development and text processing.
    
    LANGUAGE MODEL INTEGRATION:
    ===========================
    Downloaded vocabulary supports:
    - Language model training: Word frequency information
    - Vocabulary selection: Determines optimal vocabulary size
    - OOV analysis: Identifies out-of-vocabulary words in test data
    - Text normalization: Standard vocabulary for consistent processing
    
    Arguments
    ---------
    destination : str
        Directory path where librispeech-vocab.txt will be saved.
        Must be writable for file download
        
    Side Effects
    ------------
    Downloads librispeech-vocab.txt from OpenSLR repository to destination
    """
    f = "librispeech-vocab.txt"
    download_file(OPEN_SLR_11_LINK + f, destination)


def download_openslr_librispeech_lm(destination, rescoring_lm=True):
    """Downloads official LibriSpeech n-gram language models from OpenSLR repository.
    
    This function retrieves pre-trained n-gram language models that can be
    integrated with ASR systems for improved recognition accuracy. Supports
    both standard decoding and advanced rescoring configurations.
    
    LANGUAGE MODEL TYPES:
    ====================
    Available models from OpenSLR:
    - 3-gram.arpa.gz: Standard 3-gram for real-time decoding
    - 3-gram.pruned variants: Memory-optimized models for resource constraints
    - 4-gram.arpa.gz: Large 4-gram for maximum accuracy rescoring
    
    ASR INTEGRATION:
    ===============
    Downloaded models support:
    - First-pass decoding: 3-gram models for initial recognition
    - N-best rescoring: 4-gram models for accuracy improvement
    - Beam search integration: ARPA format compatible with SpeechBrain
    - Language model adaptation: Base models for domain-specific training
    
    Arguments
    ---------
    destination : str
        Directory where language model files will be downloaded and unpacked.
        Must have sufficient space for large ARPA models
        
    rescoring_lm : bool, optional
        If True, downloads 4-gram models for rescoring in addition to 3-gram.
        Default: True for maximum accuracy capability
        
    Side Effects
    ------------
    Downloads and unpacks multiple ARPA language model files to destination
    """
    os.makedirs(destination, exist_ok=True)
    for f in DataPreparationConstants.AVAILABLE_NGRAM_MODELS:
        if f.startswith("4") and not rescoring_lm:
            continue
        d = os.path.join(destination, f)
        download_file(DataPreparationConstants.OPEN_SLR_11_BASE_URL + f, d, unpack=True)


def download_sb_librispeech_lm(destination, rescoring_lm=True):
    """Download sb librispeech lm and unpack it.

    Arguments
    ---------
    destination : str
        Place to put lm.
    rescoring_lm : bool
        Also download bigger 4grams model
    """
    os.makedirs(destination, exist_ok=True)
    download_file(
        "https://www.dropbox.com/scl/fi/3fkkdlliavhveb5n3nsow/3gram_lm.arpa?rlkey=jgdrluppfut1pjminf3l3y106&dl=1",
        os.path.join(destination, "3-gram_sb.arpa"),
    )
    if rescoring_lm:
        download_file(
            "https://www.dropbox.com/scl/fi/roz46ee0ah2lvy5csno4z/4gram_lm.arpa?rlkey=2wt8ozb1mqgde9h9n9rp2yppz&dl=1",
            os.path.join(destination, "4-gram_sb.arpa"),
        )
