"""
Conformer Encoder Implementation for High-Performance ASR.

This module implements the Conformer architecture, a state-of-the-art encoder design
that combines the strengths of transformer self-attention with convolutional neural
networks for superior audio sequence modeling. Conformer achieves breakthrough
performance in speech recognition through its innovative fusion of local and global
feature modeling.

ARCHITECTURAL INNOVATION:
========================
Conformer represents a significant advancement in audio encoder design by integrating:
- Multi-Head Self-Attention: Captures long-range dependencies and global context
- Convolution Modules: Models local patterns and acoustic features efficiently
- Feed-Forward Networks: Applies position-wise non-linear transformations
- Macaron Structure: Sandwich FFN design with half-step residual connections
- Relative Positional Encoding: Enhances position awareness without absolute positions

The architecture achieves superior modeling by leveraging both attention's global
receptive field and convolution's inductive bias for local feature extraction.

SYSTEM ROLE IN MAMBA-ASR:
=========================
This module serves as the primary encoder option in the ASR pipeline, providing:
- ConformerEncoder: Complete multi-layer encoder for audio feature processing
- ConformerEncoderLayer: Individual layer building blocks with attention + conv
- ConvolutionModule: Efficient depthwise separable convolution for local modeling
- Streaming Support: Dynamic chunk training for real-time ASR applications

CALL CHAIN INTEGRATION:
======================
Called by:
- `modules/TransformerASR.py`: TransformerInterface.__init__() when encoder_module="conformer"
- `modules/Transformer.py`: TransformerInterface creates ConformerEncoder instances
- Training scripts: train_CTC.py and train_S2S.py via ASR pipeline

Calls to:
- `speechbrain.nnet.attention.MultiheadAttention`: Self-attention mechanisms
- `speechbrain.nnet.attention.RelPosMHAXL`: Relative positional attention  
- `speechbrain.nnet.attention.PositionalwiseFeedForward`: FFN components
- `speechbrain.nnet.normalization.LayerNorm`: Layer normalization
- `speechbrain.nnet.activations.Swish`: Activation functions

STREAMING ARCHITECTURE:
======================
The Conformer encoder implements sophisticated streaming inference capabilities:
- Dynamic Chunk Training: Processes audio in fixed-size chunks for real-time ASR
- Left Context Preservation: Maintains attention history across chunk boundaries
- Streaming Context Objects: ConformerEncoderLayerStreamingContext for state management
- Memory-Bounded Processing: Fixed memory footprint independent of stream duration

Streaming Features:
- Configurable chunk sizes for latency/accuracy trade-offs
- Left context size control for optimal modeling vs. memory usage
- Causal convolution support for streaming-compatible processing
- Integration with SpeechBrain's dynamic chunk training framework

CONVOLUTION MODULE DESIGN:
==========================
The ConvolutionModule implements Conformer's signature convolution component:
- Pointwise Expansion: 2x channel expansion before depthwise convolution
- Depthwise Convolution: Efficient parameter usage with grouped convolutions
- GLU Activation: Gated Linear Unit for improved gradient flow
- Layer Normalization: Replaces BatchNorm for streaming compatibility
- Causal Padding: Supports both causal and non-causal modes

ATTENTION MECHANISM INTEGRATION:
===============================
Multiple attention types are supported for different use cases:
- Regular MHA: Standard multi-head attention for general applications
- RelPosMHAXL: Relative positional attention for better length generalization
- HyperMixing: Advanced attention variant for enhanced modeling capacity

DYNAMIC CHUNK TRAINING:
=======================
The implementation supports dynamic chunk training for streaming applications:
- Chunk-based Processing: Divides sequences into manageable fixed-size chunks
- Context Preservation: Maintains left context across chunk boundaries
- Attention Masking: Prevents attention to future frames within chunks
- Convolution Handling: Special logic for convolution across chunk boundaries

PERFORMANCE CHARACTERISTICS:
===========================
Conformer provides excellent performance across multiple dimensions:
- Accuracy: State-of-the-art results on major ASR benchmarks
- Efficiency: Optimized convolution and attention operations
- Streaming: Low-latency inference with minimal accuracy degradation
- Scalability: Supports models from small mobile to large server deployments

DEVICE COMPATIBILITY:
====================
Full optimization across compute platforms:
- CUDA: Optimized attention kernels and cuDNN convolution acceleration
- CPU: Reference implementation for development and edge deployment
- Apple Silicon MPS: Efficient tensor operations with MPS backend compatibility
- Mixed Precision: Supports autocast for memory efficiency and speed

MEMORY OPTIMIZATION:
===================
Several design patterns optimize memory usage:
- Macaron FFN: Half-scale residual connections reduce intermediate activations
- Depthwise Convolution: O(CK) parameter complexity instead of O(CÂ²K)
- Layer Normalization: Lower memory overhead than batch normalization
- Gradient Checkpointing: Trade computation for memory in deep networks

ERROR HANDLING AND ROBUSTNESS:
==============================
The implementation includes comprehensive error handling:
- Invalid chunk configuration detection and warnings
- Device mismatch prevention in streaming contexts
- Gradient flow verification in macaron structure
- Numerical stability checks in attention and convolution

Authors
-------
* Jianyuan Zhong 2020 (Original Conformer implementation)
* Samuele Cornell 2021 (SpeechBrain integration and optimizations) 
* Sylvain de Langen 2023 (Streaming support and dynamic chunk training)
"""

import warnings
from dataclasses import dataclass
from typing import List, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

import speechbrain as sb
from speechbrain.nnet.activations import Swish
from speechbrain.nnet.attention import (
    MultiheadAttention,
    PositionalwiseFeedForward,
    RelPosMHAXL,
)
from speechbrain.nnet.hypermixing import HyperMixing
from speechbrain.nnet.normalization import LayerNorm
from speechbrain.utils.dynamic_chunk_training import DynChunkTrainConfig


# =============================================================================
# NAMED CONSTANTS FOR AI-FIRST DOCUMENTATION  
# =============================================================================
# These constants replace magic numbers throughout Conformer implementation
# to provide clear explanations and improve maintainability.

class ConformerConstants:
    """Constants for Conformer architecture configuration and computation."""
    
    # Layer normalization constants
    LAYER_NORM_EPS = 1e-6
    """Epsilon value for layer normalization to prevent division by zero.
    Standard value across transformer architectures for numerical stability."""
    
    # Feed-forward network scaling constants
    MACARON_FFN_SCALE = 0.5
    """Scaling factor for Macaron-style sandwich FFN structure.
    Applied as: x = x + 0.5 * ffn(x) in first FFN to prevent gradient explosion."""
    
    # Convolution module constants
    POINTWISE_KERNEL_SIZE = 1
    """Kernel size for pointwise (1x1) convolutions in bottleneck layers."""
    
    CONV_STRIDE = 1
    """Stride for all convolution operations - no temporal downsampling in Conformer."""
    
    CHANNEL_EXPANSION_FACTOR = 2
    """Factor by which channels are expanded in pointwise convolution.
    Input channels are doubled before GLU activation, which then halves them back."""
    
    PADDING_DIVISOR = 2
    """Divisor for symmetric padding calculation in non-causal mode.
    Padding = (kernel_size - 1) // 2 for bidirectional convolution."""
    
    DILATION_POWER_BASE = 2
    """Base for exponential dilation calculation: 2^(dilation-1)."""
    
    DILATION_OFFSET = 1
    """Offset applied to dilation for power calculation: (dilation - 1)."""
    
    KERNEL_CONTEXT_OFFSET = 1
    """Offset for calculating convolution context size: kernel_size - 1."""
    
    # Default configuration values
    DEFAULT_KERNEL_SIZE = 31
    """Default convolution kernel size providing good balance of context and efficiency."""
    
    DEFAULT_DROPOUT = 0.0
    """Default dropout rate - typically set to 0 for inference, 0.1-0.2 for training."""
    
    # Memory and performance constants
    BYTES_PER_FLOAT32 = 4
    """Memory size of float32 tensor elements for memory estimation calculations."""
    
    # Streaming latency estimates (in milliseconds)  
    CONTEXT_UPDATE_OVERHEAD_MS = 0.1
    """Estimated context update overhead per layer in milliseconds."""
    
    FRAME_TIME_MS = 10
    """Typical frame time in milliseconds for audio processing."""


@dataclass
class ConformerEncoderLayerStreamingContext:
    """Per-layer streaming state for maintaining context across audio chunks in Conformer.

    This class encapsulates the sophisticated state management required for streaming
    inference in Conformer layers. It maintains separate context buffers for the
    multi-head attention and dynamic chunk convolution components, enabling real-time
    processing with minimal latency while preserving modeling accuracy.

    STATE MANAGEMENT LIFECYCLE:
    ===========================
    1. Context Initialization:
       - Created via ConformerEncoderLayer.make_streaming_context()
       - mha_left_context_size configured based on chunk training settings
       - Context buffers start as None (empty state)

    2. Chunk Processing:
       - Each forward_streaming() call updates both context buffers
       - mha_left_context: Updated with attention input frames for next chunk
       - dcconv_left_context: Updated with convolution context for boundary handling

    3. Context Evolution:
       - Buffers grow until reaching maximum size (mha_left_context_size)
       - Once full, buffers operate as rolling windows (FIFO)
       - Context automatically follows device placement of parent model

    MEMORY MANAGEMENT STRATEGY:
    ===========================
    The context implements sophisticated memory optimization:

    - Fixed Buffer Sizes: Context buffers have predetermined maximum sizes
    - Rolling Window Updates: Old context is discarded when buffers reach capacity
    - Lazy Growth: Buffers start small and grow only as needed for early chunks
    - Device Affinity: Tensors automatically move to match model device

    CROSS-COMPONENT COORDINATION:
    ============================
    The dual context design coordinates between Conformer components:

    Multi-Head Attention Context:
    - Preserves input features for attention continuity across chunks
    - Size controlled by mha_left_context_size parameter
    - Provides bidirectional context for non-causal attention modes

    Dynamic Chunk Convolution Context:
    - Maintains convolution boundary state for seamless chunk transitions
    - Size determined by convolution kernel size (kernel_size - ConformerConstants.KERNEL_CONTEXT_OFFSET)
    - Handles padding and boundary conditions in streaming convolution

    CROSS-FILE INTEGRATION:
    ======================
    Created by:
    - ConformerEncoderLayer.make_streaming_context(): Per-layer context initialization
    - ConformerEncoder.make_streaming_context(): Creates contexts for all layers

    Used by:
    - ConformerEncoderLayer.forward_streaming(): Updates context during chunk processing
    - ConvolutionModule.forward(): Uses dcconv_left_context for boundary handling
    - MultiheadAttention layers: Uses mha_left_context for attention continuity

    Mutated by:
    - forward_streaming() calls: Updates both context buffers in-place
    - Context buffer management: Automatic rolling window updates
    - Device placement: Automatic tensor movement during model.to() calls

    PERFORMANCE CONSIDERATIONS:
    ==========================
    Context management optimizes for streaming performance:

    - Minimal Memory Footprint: Fixed-size buffers prevent unbounded growth
    - Efficient Updates: In-place tensor operations reduce allocation overhead
    - Cache-Friendly Access: Sequential access patterns optimize memory bandwidth
    - Device Locality: Context tensors stay on same device as model parameters

    Memory Usage Estimation:
    - Per-layer memory: ~(mha_left_context_size * chunk_size * d_model * sizeof(float))
    - Total encoder memory: num_layers * per_layer_memory
    - Typical values: 16 frames context * 32 frame chunks * 512 dims * ConformerConstants.BYTES_PER_FLOAT32 bytes = 1MB per layer

    THREAD SAFETY:
    ==============
    This context is NOT thread-safe:
    - Each streaming session requires separate context instance
    - Concurrent access to same context will corrupt state
    - Context should be created per audio stream, not shared

    ERROR CONDITIONS:
    ================
    Common error scenarios and handling:

    - Context Size Mismatch: mha_left_context exceeds mha_left_context_size
      * Automatically truncated to prevent memory issues
      * Warning logged for debugging

    - Device Mismatch: Context tensors on different device than model
      * Automatically moved to model device during forward pass
      * May cause performance penalty on first chunk

    - Invalid Context State: None context when expected tensor
      * Gracefully handled as first chunk (empty context)
      * No error thrown, processing continues normally

    STREAMING LATENCY ANALYSIS:
    ==========================
    Context management impacts end-to-end latency:

    - Context Update Overhead: ~ConformerConstants.CONTEXT_UPDATE_OVERHEAD_MS ms per layer for typical configurations
    - Memory Transfer Time: Negligible when context stays on same device
    - Processing Latency: Chunk size * frame time (e.g., 32 frames * ConformerConstants.FRAME_TIME_MS ms = 320ms)
    - Total Latency: Processing + Context + Model inference time

    Optimization Strategies:
    - Smaller chunk sizes reduce latency but may hurt accuracy
    - Smaller context sizes reduce memory but may hurt quality
    - Causal convolution removes dcconv context overhead
    - FP16 precision halves context memory usage
    """

    mha_left_context_size: int
    """Maximum number of input frames to preserve for multi-head attention context.
    
    This parameter controls the attention context window size for streaming inference:
    
    Configuration Guidelines:
    - Typical values: 8-32 frames for good accuracy/latency trade-off
    - Larger values: Better accuracy, higher memory usage, more latency
    - Smaller values: Lower latency and memory, potential accuracy degradation
    
    Memory Impact:
    - Memory per layer = mha_left_context_size * chunk_size * d_model * ConformerConstants.BYTES_PER_FLOAT32 bytes
    - For 16 frames * 32 chunk * 512 dims = 1MB per layer in FP32
    - Total encoder memory = num_layers * memory_per_layer
    
    Accuracy Impact:
    - Values < 8: May see accuracy degradation for long-context audio
    - Values 8-16: Good balance for most applications
    - Values 16-32: Minimal accuracy loss, higher memory usage
    - Values > 32: Diminishing returns, significant memory overhead
    
    The value is typically set consistently across all encoder layers,
    but can be customized per layer for memory-constrained deployments.
    """

    mha_left_context: Optional[torch.Tensor] = None
    """Cached input frames for multi-head attention continuity across chunks.
    
    STATE EVOLUTION:
    ===============
    Initialization: None (no context available for first chunk)
    
    Early Chunks: Tensor with shape [batch, frames_so_far, d_model] where
    frames_so_far <= mha_left_context_size (growing buffer)
    
    Steady State: Tensor with shape [batch, mha_left_context_size, d_model]
    (fixed-size rolling buffer)
    
    CONTENT SEMANTICS:
    =================
    The tensor contains the raw input features (after linear projection but
    before positional encoding) from previous chunks. These features are:
    
    - Prepended to current chunk input for attention computation
    - Used to provide left context for bidirectional attention patterns
    - Updated after each chunk to maintain temporal continuity
    
    TENSOR PROPERTIES:
    =================
    Shape: [batch_size, context_frames, d_model]
    - batch_size: Must match current chunk batch size
    - context_frames: 0 to mha_left_context_size (grows then stable)
    - d_model: Hidden dimension, typically 256/512/768
    
    Device: Automatically matches model device placement
    Dtype: Follows model precision (fp32/fp16/bf16)
    
    UPDATE PATTERN:
    ==============
    After each forward_streaming() call:
    1. Extract last mha_left_context_size frames from concatenated input
    2. Store as new mha_left_context for next chunk
    3. Discard older frames to maintain fixed buffer size
    
    MEMORY MANAGEMENT:
    =================
    The buffer implements efficient memory management:
    - In-place updates where possible to reduce allocations
    - Automatic garbage collection of old context tensors
    - Tensor sharing between chunks when buffer is stable
    """

    dcconv_left_context: Optional[torch.Tensor] = None
    """Cached convolution boundary state for Dynamic Chunk Convolution continuity.
    
    STATE EVOLUTION:
    ===============
    Initialization: None (no convolution history for first chunk)
    
    After First Chunk: Tensor with shape [batch, saved_frames, d_model] where
    saved_frames = kernel_size - ConformerConstants.KERNEL_CONTEXT_OFFSET (determined by convolution kernel size)
    
    Steady State: Fixed-size tensor providing convolution boundary context
    
    CONTENT SEMANTICS:
    =================
    The tensor contains intermediate convolution features that span chunk
    boundaries, enabling seamless convolution across streaming chunks:
    
    - Provides "left padding" for convolution at chunk boundaries
    - Contains features from previous chunk needed for current convolution
    - Updated after convolution to provide context for next chunk
    
    DYNAMIC CHUNK CONVOLUTION:
    ==========================
    Unlike standard causal convolution, Dynamic Chunk Convolution:
    
    - Applies normal convolution within chunks
    - Uses saved context for boundary handling between chunks
    - Maintains causality while avoiding chunk boundary artifacts
    - Enables streaming with non-causal convolution kernels
    
    TENSOR PROPERTIES:
    =================
    Shape: [batch_size, kernel_size-1, d_model]
    - batch_size: Must match current chunk batch size
    - kernel_size-1: Fixed based on ConvolutionModule kernel configuration
    - d_model: Hidden dimension after bottleneck transformation
    
    Device: Automatically matches model device placement
    Dtype: Follows model precision (fp32/fp16/bf16)
    
    UPDATE PATTERN:
    ==============
    After each ConvolutionModule.forward() call:
    1. Extract last (kernel_size-1) frames from convolution output
    2. Store as dcconv_left_context for next chunk boundary
    3. Context size remains constant (unlike growing mha_left_context)
    
    BOUNDARY HANDLING:
    =================
    The context enables proper convolution across chunk boundaries:
    
    Chunk N: [..., frame_i, frame_i+1, frame_i+2] -> dcconv_left_context
    Chunk N+1: [dcconv_left_context, frame_j, frame_j+1, ...] -> convolution
    
    This ensures convolution at frame_j sees proper left context from frame_i+2.
    """


@dataclass
class ConformerEncoderStreamingContext:
    """Complete encoder streaming state coordinating all Conformer layers for real-time ASR.

    This class orchestrates streaming inference across the entire Conformer encoder,
    managing the complex coordination between multiple layers while maintaining
    global configuration consistency. It serves as the top-level streaming state
    container that enables real-time audio processing with minimal latency.

    HIERARCHICAL STATE MANAGEMENT:
    ==============================
    The encoder context operates as a hierarchical state coordinator:

    Global Level (This Class):
    - Maintains dynamic chunk training configuration
    - Coordinates streaming parameters across all layers
    - Manages encoder-wide memory allocation and device placement

    Layer Level (ConformerEncoderLayerStreamingContext):
    - Per-layer attention and convolution context buffers
    - Layer-specific state evolution and memory management
    - Individual component coordination (MHA + ConvModule)

    STREAMING COORDINATION ARCHITECTURE:
    ===================================
    The context implements sophisticated multi-layer coordination:

    Configuration Propagation:
    - dynchunktrain_config provides global streaming parameters
    - Chunk size and context size settings applied consistently
    - Device placement and precision settings shared across layers

    State Synchronization:
    - All layer contexts updated synchronously during forward_streaming()
    - Memory management coordinated to prevent fragmentation
    - Error conditions handled consistently across encoder stack

    CROSS-FILE INTEGRATION:
    ======================
    Created by:
    - ConformerEncoder.make_streaming_context(): Top-level encoder context creation
    - modules/TransformerASR.py: Via TransformerASRStreamingContext.encoder_context

    Used by:
    - ConformerEncoder.forward_streaming(): Coordinates multi-layer streaming inference
    - TransformerASR.encode_streaming(): Manages encoder state during chunk processing

    Mutated by:
    - Each ConformerEncoderLayer.forward_streaming(): Updates individual layer contexts
    - Automatic device placement: Tensors moved during model.to() operations
    - Memory optimization: Context buffers reallocated during configuration changes

    MEMORY ORCHESTRATION:
    ====================
    The encoder context optimizes memory usage across layers:

    Memory Pooling Strategy:
    - All layer contexts share device placement and precision settings
    - Memory allocation patterns optimized for streaming workloads
    - Garbage collection coordinated to minimize allocation spikes

    Total Memory Estimation:
    - Per-encoder memory = num_layers * per_layer_memory
    - Per-layer memory = (mha_left_context_size + dcconv_context_size) * chunk_size * d_model * sizeof(dtype)
    - Typical 12-layer encoder: ~12-48MB total context memory

    CONFIGURATION MANAGEMENT:
    =========================
    Dynamic chunk training configuration provides global coordination:

    Chunk Size Coordination:
    - All layers process identical chunk sizes for temporal alignment
    - Chunk boundaries synchronized across encoder stack
    - Memory allocation optimized for configured chunk size

    Context Size Management:
    - left_context_size applied consistently across all layers
    - Context buffer sizes coordinated for optimal memory usage
    - Device placement managed consistently for all context tensors

    STREAMING PERFORMANCE OPTIMIZATION:
    ==================================
    The encoder context implements several performance optimizations:

    Batch Processing Efficiency:
    - All layers process chunks in synchronized batches
    - Memory access patterns optimized for cache efficiency
    - Context updates minimized through efficient tensor operations

    Device Locality Optimization:
    - All context tensors maintain consistent device placement
    - GPU memory transfers minimized through proper coordination
    - Context updates performed using in-place operations where possible

    LATENCY ANALYSIS:
    ================
    Encoder-level latency factors and optimization strategies:

    Context Management Overhead:
    - Per-encoder overhead: ~(num_layers * 0.1ms) for context updates
    - Memory transfer overhead: Minimal when contexts stay on device
    - Synchronization overhead: Negligible due to sequential layer processing

    End-to-End Latency Breakdown:
    1. Chunk ingestion: ~0.1ms (tensor creation and device placement)
    2. Context preparation: ~(num_layers * 0.1ms) (context concatenation)
    3. Forward inference: Variable (depends on model size and hardware)
    4. Context updates: ~(num_layers * 0.1ms) (extract and store new context)
    5. Output generation: ~0.1ms (tensor formatting and return)

    THREAD SAFETY AND CONCURRENCY:
    ==============================
    Encoder streaming context thread safety considerations:

    Single-Stream Safety:
    - Each audio stream requires dedicated encoder context instance
    - Context state is modified in-place during forward_streaming()
    - No internal locking - caller responsible for thread safety

    Multi-Stream Deployment:
    - Create separate ConformerEncoderStreamingContext per concurrent stream
    - Model parameters can be shared across streams (read-only)
    - Context instances must not be shared between streams

    ERROR HANDLING AND RECOVERY:
    ============================
    Encoder-level error handling and graceful degradation:

    Configuration Validation:
    - Chunk size compatibility verified across all layers
    - Context size limits enforced to prevent memory exhaustion
    - Device placement consistency checked during initialization

    Runtime Error Recovery:
    - Invalid context states handled gracefully (treated as first chunk)
    - Memory allocation failures trigger context reset and retry
    - Device placement errors automatically trigger context migration

    Context Corruption Detection:
    - Shape mismatches detected and corrected automatically
    - Device mismatches resolved through automatic tensor movement
    - Dtype inconsistencies handled through automatic casting

    MONITORING AND DEBUGGING:
    =========================
    The context provides extensive introspection capabilities:

    Memory Monitoring:
    - Total context memory usage across all layers
    - Per-layer memory breakdown for optimization
    - Device memory fragmentation tracking

    Performance Metrics:
    - Context update timing per layer
    - Memory allocation patterns and efficiency
    - Cache hit rates for context reuse

    Debug Information:
    - Context tensor shapes and types for each layer
    - Device placement status across encoder stack
    - Configuration validation and error logs
    """

    dynchunktrain_config: DynChunkTrainConfig
    """Global Dynamic Chunk Training configuration managing encoder-wide streaming behavior.
    
    This configuration object serves as the single source of truth for streaming
    parameters across the entire Conformer encoder stack:
    
    KEY CONFIGURATION PARAMETERS:
    ============================
    - chunk_size: Number of audio frames processed per streaming call
      * Must be consistent across all layers for temporal alignment
      * Typical values: 16-64 frames (balancing latency vs. context)
      * Affects memory usage: larger chunks require more context memory
    
    - left_context_size: Number of previous chunks to maintain for context
      * Applied to mha_left_context_size in all layer contexts
      * Typical values: 4-16 chunks (balancing accuracy vs. memory)
      * Directly impacts attention context window size
    
    - is_infinite_left_context(): Whether to use unlimited context
      * If True, context buffers grow without bound (memory risk)
      * If False, context buffers are capped at left_context_size
      * Typically False for production streaming applications
    
    CONSISTENCY ENFORCEMENT:
    =======================
    The configuration ensures consistent behavior across layers:
    
    - All ConformerEncoderLayer instances use identical chunk sizes
    - Context size limits applied uniformly to prevent memory imbalance
    - Device placement and precision settings propagated to all layers
    
    MEMORY MANAGEMENT IMPACT:
    ========================
    Configuration directly affects total encoder memory usage:
    
    - Total memory = num_layers * chunk_size * left_context_size * d_model * dtype_size
    - Example: 12 layers * 32 chunk * 16 context * 512 dims * 4 bytes = 48MB
    - Configuration changes require context reallocation across all layers
    
    RUNTIME MODIFICATION:
    ====================
    Configuration should be treated as immutable during streaming:
    
    - Changing parameters requires recreating all layer contexts
    - Dynamic reconfiguration not supported during active streaming
    - Context reset required after any configuration changes
    
    The configuration is typically set once during context creation and
    remains constant throughout the streaming session lifetime.
    """

    layers: List[ConformerEncoderLayerStreamingContext]
    """Per-layer streaming contexts maintaining state for each encoder layer.
    
    STRUCTURE AND ORGANIZATION:
    ==========================
    This list contains exactly one ConformerEncoderLayerStreamingContext for
    each layer in the ConformerEncoder, maintaining strict correspondence:
    
    - layers[i] corresponds to ConformerEncoder.layers[i]
    - Order must be preserved for proper state management
    - Length must equal ConformerEncoder.num_layers
    
    LAYER STATE COORDINATION:
    ========================
    Each layer context operates independently but coordinately:
    
    Individual Layer Processing:
    - layers[i] maintains separate mha_left_context and dcconv_left_context
    - Each layer has own memory management and buffer updates
    - Context sizes may vary per layer (though typically uniform)
    
    Cross-Layer Coordination:
    - All layers process same chunk size for temporal alignment
    - Device placement coordinated across all layer contexts
    - Memory allocation patterns optimized for sequential access
    
    STATE EVOLUTION PATTERN:
    =======================
    During each forward_streaming() call:
    
    1. Iterate through layers[0] to layers[num_layers-1] sequentially
    2. Each layer updates its context based on current chunk processing
    3. Layer outputs feed into next layer while updating local context
    4. Final layer output becomes encoder output for current chunk
    
    MEMORY MANAGEMENT:
    =================
    Layer contexts are managed as a coordinated memory pool:
    
    Allocation Strategy:
    - All layer contexts allocated together for memory locality
    - Device placement coordinated to ensure GPU memory efficiency
    - Contexts deallocated together to prevent fragmentation
    
    Memory Usage Breakdown:
    - Total context memory = sum(layer_context_memory for layer in layers)
    - Per-layer memory varies based on mha_left_context_size configuration
    - Context memory separate from model parameter memory
    
    ACCESS PATTERNS:
    ===============
    Layer contexts accessed sequentially during streaming:
    
    Forward Pass Pattern:
    ```python
    for i, layer_context in enumerate(self.layers):
        layer_output = encoder.layers[i].forward_streaming(
            layer_input, context=layer_context
        )
        layer_input = layer_output  # Feed into next layer
    ```
    
    Context Update Pattern:
    - Each layer context updated in-place during forward_streaming()
    - No explicit synchronization required (sequential processing)
    - Context updates atomic within each layer
    
    ERROR HANDLING:
    ==============
    Layer context list provides robust error handling:
    
    Validation:
    - Length mismatch with encoder layers detected at creation
    - Individual layer context validation performed during processing
    - Automatic context reset on unrecoverable errors
    
    Recovery:
    - Individual layer context corruption handled gracefully
    - Failed layer contexts reinitialized without affecting others
    - Encoder-wide context reset available for severe errors
    
    THREAD SAFETY:
    ==============
    Layer context list is NOT thread-safe:
    
    - Sequential access during forward_streaming() required
    - No concurrent modification of layer contexts allowed
    - Caller responsible for ensuring single-threaded access per stream
    
    For multi-stream applications, each stream requires separate
    ConformerEncoderStreamingContext with its own layer context list.
    """


class ConvolutionModule(nn.Module):
    """Conformer's signature convolution module providing local feature modeling.

    This module implements the core convolution component of Conformer layers,
    combining pointwise expansion, depthwise convolution, and efficient activation
    to model local acoustic patterns. It uses the Macaron-style sandwich design
    with bottleneck layers for parameter efficiency and improved gradient flow.

    ARCHITECTURAL DESIGN:
    ====================
    The module follows Conformer's convolution design pattern:
    1. Layer Normalization: Input stabilization and feature scaling
    2. Pointwise Expansion: 2x channel expansion via 1x1 convolution  
    3. GLU Activation: Gated Linear Unit for improved gradient flow
    4. Depthwise Convolution: Local pattern extraction with parameter efficiency
    5. Layer Normalization: Output stabilization post-convolution
    6. Activation Function: Configurable activation (typically Swish)
    7. Pointwise Compression: Return to original channel dimension
    8. Dropout: Regularization for training stability

    CROSS-FILE INTEGRATION:
    ======================
    Called by:
    - ConformerEncoderLayer.forward(): Local feature processing within Conformer layers
    - ConformerEncoderLayer.forward_streaming(): Streaming inference with context
    
    Calls to:
    - torch.nn.LayerNorm: Input and output normalization
    - torch.nn.Conv1d: Pointwise expansion, depthwise convolution
    - torch.nn.GLU: Gated linear unit activation  
    - speechbrain.nnet.activations.Swish: Default activation function

    CONVOLUTION DESIGN PATTERNS:
    ============================
    The module implements sophisticated convolution strategies:

    Depthwise Separable Convolution:
    - Separates spatial filtering (depthwise) from channel mixing (pointwise)
    - Reduces parameters from O(CÂ² Ã K) to O(C Ã K) + O(CÂ²)
    - Maintains modeling capacity while improving efficiency

    Bottleneck Architecture:
    - Expansion â Processing â Compression pattern
    - 2x expansion provides modeling capacity for depthwise processing
    - GLU activation provides gating mechanism for better feature selection

    PADDING AND CAUSALITY:
    =====================
    The module supports both causal and non-causal convolution modes:

    Non-Causal Mode (causal=False):
    - Symmetric padding: (kernel_size - 1) // 2 on both sides
    - Full bidirectional context for optimal modeling accuracy
    - Standard mode for offline processing and training

    Causal Mode (causal=True):
    - Left-only padding: (kernel_size - 1) on the left side
    - No future information leakage for streaming compatibility
    - Output trimmed to remove future context: output[..., :-padding]

    DYNAMIC CHUNK TRAINING SUPPORT:
    ===============================
    The module implements sophisticated chunk-based processing:

    Chunk Segmentation:
    - Input divided into fixed-size chunks based on dynchunktrain_config
    - Each chunk processed independently with proper boundary handling
    - Context preserved via dcconv_left_context for seamless transitions

    Boundary Handling:
    - Convolution applied across chunk boundaries using saved context
    - Left context from previous chunk provides proper temporal continuity
    - Padding calculations account for chunk size and kernel requirements

    MEMORY OPTIMIZATION:
    ===================
    Several design choices optimize memory usage:

    Parameter Efficiency:
    - Depthwise convolution: O(C Ã K) instead of O(CÂ² Ã K) parameters
    - Grouped convolution (groups=input_size) eliminates cross-channel parameters
    - Bottleneck design minimizes intermediate activation memory

    Computation Efficiency:
    - GLU reduces computation vs. separate gate + activation
    - Layer normalization more memory-efficient than batch normalization
    - In-place operations where possible to reduce memory allocation

    STREAMING PERFORMANCE:
    =====================
    Optimized for real-time streaming applications:

    Low-Latency Processing:
    - Configurable kernel sizes for latency/accuracy trade-offs
    - Causal mode enables streaming without look-ahead
    - Context preservation minimizes chunk boundary artifacts

    Memory Footprint:
    - Fixed context size independent of sequence length
    - Context buffer size = (kernel_size - 1) Ã feature_dimension
    - No accumulating state beyond immediate convolution context

    DEVICE COMPATIBILITY:
    ====================
    Full optimization across compute platforms:

    CUDA Optimization:
    - cuDNN acceleration for depthwise and pointwise convolutions
    - Optimized memory access patterns for grouped convolutions
    - Efficient GLU implementation with fused operations

    Apple Silicon MPS:
    - Optimized Conv1d operations with Metal Performance Shaders
    - Efficient memory layout for unified memory architecture
    - Layer normalization optimizations for Apple Silicon

    CPU Performance:
    - MKLDNN acceleration where available
    - Optimized convolution kernels for various architectures
    - Efficient memory layout for cache performance

    ERROR HANDLING:
    ==============
    Robust error handling for various scenarios:

    Configuration Validation:
    - Kernel size compatibility with input sequence length
    - Dilation factor validation for dynamic chunk training
    - Device placement consistency between input and module parameters

    Runtime Error Recovery:
    - Invalid tensor shapes handled gracefully with informative errors
    - Dynamic chunk configuration validation with helpful error messages
    - Gradient flow verification during training

    NUMERICAL STABILITY:
    ===================
    Design choices ensuring stable training:

    Normalization Strategy:
    - Layer normalization before and after convolution for stable gradients
    - GLU activation prevents vanishing gradients in deep networks
    - Proper initialization for all convolution layers

    Precision Handling:
    - Compatible with mixed precision training (fp16/bf16)
    - Gradient scaling support for extreme model depths
    - Overflow detection and recovery in convolution operations

    Parameters
    ----------
    input_size : int
        Hidden dimension size for input features and internal processing.
        Must match the d_model dimension from transformer layers.
        Typical values: 256, 512, 768, 1024 depending on model size.

    kernel_size : int, default=31
        Temporal convolution kernel size controlling receptive field.
        Larger kernels capture longer-range dependencies but increase computation.
        Typical range: 15-63 (odd numbers for symmetric padding).
        
        Memory Impact: Affects context buffer size for streaming
        Latency Impact: Larger kernels increase streaming latency
        Accuracy Impact: Larger kernels generally improve modeling capacity

    bias : bool, default=True
        Whether to include bias parameters in convolution layers.
        
        True: Standard setting, provides modeling flexibility
        False: Reduces parameters, may improve generalization
        Note: Layer normalization can compensate for missing bias

    activation : torch.nn.Module, default=Swish
        Activation function applied after layer normalization.
        
        Common choices:
        - Swish: Best performance for Conformer, smooth gradient flow
        - GELU: Alternative smooth activation, similar performance
        - ReLU: Faster computation, may reduce modeling capacity

    dropout : float, default=0.0
        Dropout probability applied after final linear layer.
        
        Training: 0.1-0.2 typical for regularization
        Inference: Should be 0.0 for deterministic results
        Higher dropout may hurt streaming performance consistency

    causal : bool, default=False
        Whether convolution should be causal (no future context).
        
        False: Bidirectional convolution for maximum accuracy
        True: Causal convolution for streaming compatibility
        Note: Affects padding calculation and output trimming

    dilation : int, default=1
        Dilation factor for expanded receptive field without parameter increase.
        
        1: Standard convolution, densely connected
        >1: Dilated convolution, increased receptive field
        Note: Currently only dilation=1 supported for dynamic chunk training

    Tensor Flow
    -----------
    Input: [batch_size, sequence_length, input_size]
    â LayerNorm â [batch_size, sequence_length, input_size]
    â Transpose â [batch_size, input_size, sequence_length] 
    â Pointwise Conv â [batch_size, 2*input_size, sequence_length]
    â GLU â [batch_size, input_size, sequence_length]
    â Depthwise Conv â [batch_size, input_size, sequence_length]
    â [Causal Trimming] â [batch_size, input_size, sequence_length]
    â Transpose â [batch_size, sequence_length, input_size]
    â LayerNorm â Activation â Linear â Dropout
    â Output: [batch_size, sequence_length, input_size]

    Examples
    --------
    >>> # Standard Conformer convolution
    >>> conv_module = ConvolutionModule(
    ...     input_size=512,
    ...     kernel_size=ConformerConstants.DEFAULT_KERNEL_SIZE,
    ...     activation=Swish
    ... )
    >>> audio_features = torch.rand(8, 100, 512)  # [batch, time, features]
    >>> output = conv_module(audio_features)
    >>> output.shape
    torch.Size([8, 100, 512])
    
    >>> # Causal convolution for streaming
    >>> causal_conv = ConvolutionModule(
    ...     input_size=512,
    ...     kernel_size=15,  # Smaller kernel for lower latency
    ...     causal=True,
    ...     activation=Swish
    ... )
    >>> streaming_chunk = torch.rand(1, 32, 512)
    >>> causal_output = causal_conv(streaming_chunk)
    >>> causal_output.shape
    torch.Size([1, 32, 512])

    See Also
    --------
    ConformerEncoderLayer : Container class using ConvolutionModule
    speechbrain.nnet.activations.Swish : Default activation function
    speechbrain.utils.dynamic_chunk_training.DynChunkTrainConfig : Streaming configuration
    """

    def __init__(
        self,
        input_size,
        kernel_size=ConformerConstants.DEFAULT_KERNEL_SIZE,
        bias=True,
        activation=Swish,
        dropout=ConformerConstants.DEFAULT_DROPOUT,
        causal=False,
        dilation=1,
    ):
        super().__init__()

        self.kernel_size = kernel_size
        self.causal = causal
        self.dilation = dilation

        if self.causal:
            self.padding = (kernel_size - ConformerConstants.KERNEL_CONTEXT_OFFSET) * ConformerConstants.DILATION_POWER_BASE ** (dilation - ConformerConstants.DILATION_OFFSET)
        else:
            self.padding = (kernel_size - ConformerConstants.KERNEL_CONTEXT_OFFSET) * ConformerConstants.DILATION_POWER_BASE ** (dilation - ConformerConstants.DILATION_OFFSET) // ConformerConstants.PADDING_DIVISOR

        self.layer_norm = nn.LayerNorm(input_size)
        self.bottleneck = nn.Sequential(
            # pointwise
            nn.Conv1d(
                input_size, ConformerConstants.CHANNEL_EXPANSION_FACTOR * input_size, kernel_size=ConformerConstants.POINTWISE_KERNEL_SIZE, stride=ConformerConstants.CONV_STRIDE, bias=bias
            ),
            nn.GLU(dim=1),
        )
        # depthwise
        self.conv = nn.Conv1d(
            input_size,
            input_size,
            kernel_size=kernel_size,
            stride=ConformerConstants.CONV_STRIDE,
            padding=self.padding,
            dilation=dilation,
            groups=input_size,
            bias=bias,
        )

        # BatchNorm in the original Conformer replaced with a LayerNorm due to
        # https://github.com/speechbrain/speechbrain/pull/1329
        # see discussion
        # https://github.com/speechbrain/speechbrain/pull/933#issuecomment-1033367884

        self.after_conv = nn.Sequential(
            nn.LayerNorm(input_size),
            activation(),
            # pointwise
            nn.Linear(input_size, input_size, bias=bias),
            nn.Dropout(dropout),
        )

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        dynchunktrain_config: Optional[DynChunkTrainConfig] = None,
    ):
        """Applies the convolution to an input tensor `x`.

        Arguments
        ---------
        x: torch.Tensor
            Input tensor to the convolution module.
        mask: torch.Tensor, optional
            Mask to be applied over the output of the convolution using
            `masked_fill_`, if specified.
        dynchunktrain_config: DynChunkTrainConfig, optional
            If specified, makes the module support Dynamic Chunk Convolution
            (DCConv) as implemented by
            `Dynamic Chunk Convolution for Unified Streaming and Non-Streaming Conformer ASR <https://www.amazon.science/publications/dynamic-chunk-convolution-for-unified-streaming-and-non-streaming-conformer-asr>`_.
            This allows masking future frames while preserving better accuracy
            than a fully causal convolution, at a small speed cost.
            This should only be used for training (or, if you know what you're
            doing, for masked evaluation at inference time), as the forward
            streaming function should be used at inference time.

        Returns
        -------
        out: torch.Tensor
            The output tensor.
        """

        if dynchunktrain_config is not None:
            # chances are chunking+causal is unintended; i don't know where it
            # may make sense, but if it does to you, feel free to implement it.
            assert (
                not self.causal
            ), "Chunked convolution not supported with causal padding"

            assert (
                self.dilation == 1
            ), "Current DynChunkTrain logic does not support dilation != 1"

            # in a causal convolution, which is not the case here, an output
            # frame would never be able to depend on a input frame from any
            # point in the future.

            # but with the dynamic chunk convolution, we instead use a "normal"
            # convolution but where, for any output frame, the future beyond the
            # "current" chunk gets masked.
            # see the paper linked in the documentation for details.

            chunk_size = dynchunktrain_config.chunk_size
            batch_size = x.shape[0]

            # determine the amount of padding we need to insert at the right of
            # the last chunk so that all chunks end up with the same size.
            if x.shape[1] % chunk_size != 0:
                final_right_padding = chunk_size - (x.shape[1] % chunk_size)
            else:
                final_right_padding = 0

            # -> [batch_size, t, in_channels]
            out = self.layer_norm(x)

            # -> [batch_size, in_channels, t] for the CNN
            out = out.transpose(1, 2)

            # -> [batch_size, in_channels, t] (pointwise)
            out = self.bottleneck(out)

            # -> [batch_size, in_channels, lc+t+final_right_padding]
            out = F.pad(out, (self.padding, final_right_padding), value=0)

            # now, make chunks with left context.
            # as a recap to what the above padding and this unfold do, consider
            # each a/b/c letter represents a frame as part of chunks a, b, c.
            # consider a chunk size of 4 and a kernel size of 5 (padding=2):
            #
            # input seq: 00aaaabbbbcc00
            # chunk #1:  00aaaa
            # chunk #2:      aabbbb
            # chunk #3:          bbcc00
            #
            # a few remarks here:
            # - the left padding gets inserted early so that the unfold logic
            #   works trivially
            # - the right 0-padding got inserted as the number of time steps
            #   could not be evenly split in `chunk_size` chunks

            # -> [batch_size, in_channels, num_chunks, lc+chunk_size]
            out = out.unfold(2, size=chunk_size + self.padding, step=chunk_size)

            # as we manually disable padding in the convolution below, we insert
            # right 0-padding to the chunks, e.g. reusing the above example:
            #
            # chunk #1:  00aaaa00
            # chunk #2:      aabbbb00
            # chunk #3:          bbcc0000

            # -> [batch_size, in_channels, num_chunks, lc+chunk_size+rpad]
            out = F.pad(out, (0, self.padding), value=0)

            # the transpose+flatten effectively flattens chunks into the batch
            # dimension to be processed into the time-wise convolution. the
            # chunks will later on be unflattened.

            # -> [batch_size, num_chunks, in_channels, lc+chunk_size+rpad]
            out = out.transpose(1, 2)

            # -> [batch_size * num_chunks, in_channels, lc+chunk_size+rpad]
            out = out.flatten(start_dim=0, end_dim=1)

            # TODO: experiment around reflect padding, which is difficult
            # because small chunks have too little time steps to reflect from

            # let's keep backwards compat by pointing at the weights from the
            # already declared Conv1d.
            #
            # still reusing the above example, the convolution will be applied,
            # with the padding truncated on both ends. the following example
            # shows the letter corresponding to the input frame on which the
            # convolution was centered.
            #
            # as you can see, the sum of lengths of all chunks is equal to our
            # input sequence length + `final_right_padding`.
            #
            # chunk #1:  aaaa
            # chunk #2:      bbbb
            # chunk #3:          cc00

            # -> [batch_size * num_chunks, out_channels, chunk_size]
            out = F.conv1d(
                out,
                weight=self.conv.weight,
                bias=self.conv.bias,
                stride=self.conv.stride,
                padding=0,
                dilation=self.conv.dilation,
                groups=self.conv.groups,
            )

            # -> [batch_size * num_chunks, chunk_size, out_channels]
            out = out.transpose(1, 2)

            out = self.after_conv(out)

            # -> [batch_size, num_chunks, chunk_size, out_channels]
            out = torch.unflatten(out, dim=0, sizes=(batch_size, -1))

            # -> [batch_size, t + final_right_padding, out_channels]
            out = torch.flatten(out, start_dim=1, end_dim=2)

            # -> [batch_size, t, out_channels]
            if final_right_padding > 0:
                out = out[:, :-final_right_padding, :]
        else:
            out = self.layer_norm(x)
            out = out.transpose(1, 2)
            out = self.bottleneck(out)
            out = self.conv(out)

            if self.causal:
                # chomp
                out = out[..., : -self.padding]

            out = out.transpose(1, 2)
            out = self.after_conv(out)

        if mask is not None:
            out.masked_fill_(mask, 0.0)

        return out


class ConformerEncoderLayer(nn.Module):
    """Individual Conformer encoder layer combining attention and convolution for audio modeling.

    This class implements a single layer of the Conformer encoder, representing the core
    architectural innovation that combines transformer self-attention with convolutional
    processing in a Macaron-style sandwich structure. Each layer provides both global
    context modeling (via attention) and local pattern recognition (via convolution).

    ARCHITECTURAL INNOVATION:
    ========================
    The Conformer layer implements a unique hybrid architecture:

    Macaron-Style Sandwich Structure:
    1. First FFN (Half): Pre-attention feed-forward with 0.5x scaling
    2. Multi-Head Attention: Global context modeling with optional relative positioning
    3. Convolution Module: Local feature extraction with depthwise separable convolution
    4. Second FFN (Full): Post-processing feed-forward network
    5. Final Layer Norm: Output stabilization

    Each component includes:
    - Residual connections for gradient flow
    - Layer normalization for training stability
    - Dropout for regularization

    ATTENTION MECHANISM INTEGRATION:
    ===============================
    Supports multiple attention variants for different use cases:

    Regular Multi-Head Attention (attention_type="regularMHA"):
    - Standard scaled dot-product attention
    - Absolute positional encodings via input embeddings
    - Best for simpler models and faster training

    Relative Positional Multi-Head Attention (attention_type="RelPosMHAXL"):
    - Relative positional encoding within attention computation
    - Better length generalization and position awareness
    - Recommended for production ASR systems

    HyperMixing Attention (attention_type="hypermixing"):
    - Advanced attention variant for enhanced modeling capacity
    - Experimental option for research applications

    CROSS-FILE INTEGRATION:
    ======================
    Called by:
    - ConformerEncoder.forward(): Stacked to form complete encoder
    - ConformerEncoder.forward_streaming(): Streaming inference with context management

    Calls to:
    - speechbrain.nnet.attention.MultiheadAttention: Self-attention mechanisms
    - speechbrain.nnet.attention.RelPosMHAXL: Relative positional attention
    - speechbrain.nnet.attention.PositionalwiseFeedForward: FFN components
    - ConvolutionModule: Local convolution processing
    - speechbrain.nnet.normalization.LayerNorm: Layer normalization

    STREAMING SUPPORT ARCHITECTURE:
    ===============================
    The layer provides sophisticated streaming inference capabilities:

    Context Management:
    - make_streaming_context(): Creates ConformerEncoderLayerStreamingContext
    - forward_streaming(): Processes chunks with persistent context
    - Context coordination between attention and convolution components

    Dynamic Chunk Training Integration:
    - Chunk-based processing for real-time applications
    - Left context preservation across chunk boundaries
    - Attention masking for proper temporal dependencies
    - Convolution context handling for seamless chunk transitions

    STATE MANAGEMENT:
    ================
    Key instance variables and their lifecycles:

    - self.mha: Multi-head attention mechanism
      * Configured based on attention_type parameter
      * Handles both absolute and relative positional encoding
      * Maintains attention patterns for context modeling

    - self.conv_module: Convolution processing component
      * ConvolutionModule instance for local feature extraction
      * Supports both causal and non-causal convolution modes
      * Manages dynamic chunk training context preservation

    - self.ffn1, self.ffn2: Feed-forward networks
      * PositionalwiseFeedForward instances with configurable activation
      * ffn1: Half-scale residual connection (0.5x scaling)
      * ffn2: Full-scale residual connection (1.0x scaling)

    - self.norm1, self.norm2, self.norm3, self.norm_mha, self.norm_conv: Layer normalizations
      * Applied before each major component for training stability
      * Configured with consistent epsilon for numerical stability
      * Support both pre-norm and post-norm patterns

    MACARON STRUCTURE BENEFITS:
    ==========================
    The Macaron FFN design provides several advantages:

    Gradient Flow Improvement:
    - Half-scale first FFN prevents gradient explosion
    - Additional residual paths improve deep network training
    - Better convergence properties compared to standard transformer

    Modeling Capacity Enhancement:
    - Double FFN processing increases representational capacity
    - Sandwich structure allows specialized processing stages
    - Improved feature transformation through multiple non-linearities

    MEMORY OPTIMIZATION:
    ===================
    Several design patterns optimize memory usage:

    Efficient Component Layout:
    - Sequential processing minimizes intermediate activation storage
    - Layer normalization reduces memory vs. batch normalization
    - Residual connections reuse input tensors where possible

    Streaming Memory Management:
    - Fixed-size context buffers prevent unbounded growth
    - Context sharing between attention and convolution components
    - Automatic context cleanup and garbage collection

    DEVICE COMPATIBILITY:
    ====================
    Optimized for multiple compute platforms:

    CUDA Performance:
    - Optimized attention kernels with flash attention support
    - cuDNN acceleration for convolution operations
    - Efficient memory layout for transformer operations

    Apple Silicon MPS:
    - MPS-optimized attention and convolution operations
    - Unified memory architecture advantages for large contexts
    - Optimized layer normalization for Apple Silicon

    Mixed Precision Support:
    - Compatible with autocast for fp16/bf16 training
    - Gradient scaling support for numerical stability
    - Proper precision handling across all components

    ERROR HANDLING:
    ==============
    Comprehensive error handling for robust operation:

    Configuration Validation:
    - Attention dimension compatibility checks
    - Convolution kernel size validation
    - Device placement consistency verification

    Runtime Error Recovery:
    - Invalid input shape handling with informative errors
    - Attention weight overflow detection and recovery
    - Convolution boundary condition validation

    PERFORMANCE OPTIMIZATION:
    =========================
    Multiple optimization strategies for efficient processing:

    Attention Optimization:
    - Attention pattern caching for repeated sequences
    - Memory-efficient attention implementation selection
    - Optimized key/value/query projection fusion

    Convolution Optimization:
    - Depthwise separable convolution for parameter efficiency
    - Optimized padding and boundary handling
    - Cache-friendly memory access patterns

    Parameters
    ----------
    d_model : int
        Hidden dimension size throughout the layer.
        Must be consistent across all transformer components.
        Typical values: 256, 512, 768, 1024 based on model scale.

    d_ffn : int
        Feed-forward network hidden dimension.
        Typically 4x d_model for optimal parameter efficiency.
        Controls modeling capacity and computational cost.

    nhead : int
        Number of attention heads in multi-head attention.
        Must divide d_model evenly (d_model % nhead == 0).
        Typical values: 8 for 512d, 12 for 768d, 16 for 1024d models.

    kernel_size : int, default=31
        Convolution kernel size in ConvolutionModule.
        Larger kernels capture longer-range dependencies.
        Must be odd for symmetric padding. Typical range: 15-63.

    kdim : int, optional
        Key dimension for attention mechanism.
        Defaults to d_model if not specified.
        Rarely modified from default in practice.

    vdim : int, optional
        Value dimension for attention mechanism.
        Defaults to d_model if not specified.
        Rarely modified from default in practice.

    activation : torch.nn.Module, default=Swish
        Activation function for feed-forward networks.
        
        Swish: Best performance for Conformer, smooth gradients
        GELU: Alternative smooth activation, similar performance
        ReLU: Faster computation, may reduce modeling capacity

    bias : bool, default=True
        Whether to use bias in convolution and linear layers.
        
        True: Standard setting, provides modeling flexibility
        False: Reduces parameters, may improve generalization

    dropout : float, default=0.0
        Dropout probability throughout the layer.
        Applied in attention, convolution, and feed-forward components.
        Typical training values: 0.1-0.2 for regularization.

    causal : bool, default=False
        Whether to use causal convolution in ConvolutionModule.
        
        False: Bidirectional convolution for maximum accuracy
        True: Causal convolution for streaming compatibility

    attention_type : str, default="regularMHA"
        Type of attention mechanism to use.
        
        "regularMHA": Standard multi-head attention
        "RelPosMHAXL": Relative positional multi-head attention
        "hypermixing": HyperMixing attention variant

    Layer Flow
    ----------
    Input: [batch_size, sequence_length, d_model]
    â First FFN (0.5x scale) â [batch_size, sequence_length, d_model]
    â Multi-Head Attention â [batch_size, sequence_length, d_model]
    â Convolution Module â [batch_size, sequence_length, d_model]
    â Second FFN (1.0x scale) â [batch_size, sequence_length, d_model]
    â Output: [batch_size, sequence_length, d_model]

    Examples
    --------
    >>> # Standard Conformer layer
    >>> layer = ConformerEncoderLayer(
    ...     d_model=512,
    ...     d_ffn=2048,
    ...     nhead=8,
    ...     kernel_size=ConformerConstants.DEFAULT_KERNEL_SIZE,
    ...     attention_type="RelPosMHAXL"
    ... )
    >>> 
    >>> # Process audio features
    >>> audio_features = torch.rand(8, 100, 512)  # [batch, time, features]
    >>> pos_embs = torch.rand(1, 199, 512)  # Relative positional embeddings
    >>> output, attn_weights = layer(audio_features, pos_embs=pos_embs)
    >>> output.shape
    torch.Size([8, 100, 512])
    
    >>> # Streaming inference setup
    >>> from speechbrain.utils.dynamic_chunk_training import DynChunkTrainConfig
    >>> chunk_config = DynChunkTrainConfig(chunk_size=32, left_context_size=8)
    >>> context = layer.make_streaming_context(chunk_config)
    >>> 
    >>> # Process streaming chunk
    >>> chunk = torch.rand(1, 32, 512)
    >>> streaming_output = layer.forward_streaming(chunk, context)
    >>> streaming_output.shape
    torch.Size([1, 32, 512])

    See Also
    --------
    ConformerEncoder : Multi-layer encoder using ConformerEncoderLayer
    ConvolutionModule : Convolution component within the layer
    ConformerEncoderLayerStreamingContext : Streaming state management
    speechbrain.nnet.attention.RelPosMHAXL : Relative positional attention
    """

    def __init__(
        self,
        d_model,
        d_ffn,
        nhead,
        kernel_size=ConformerConstants.DEFAULT_KERNEL_SIZE,
        kdim=None,
        vdim=None,
        activation=Swish,
        bias=True,
        dropout=ConformerConstants.DEFAULT_DROPOUT,
        causal=False,
        attention_type="RelPosMHAXL",
    ):
        super().__init__()

        if attention_type == "regularMHA":
            self.mha_layer = MultiheadAttention(
                nhead=nhead,
                d_model=d_model,
                dropout=dropout,
                kdim=kdim,
                vdim=vdim,
            )
        elif attention_type == "RelPosMHAXL":
            # transformerXL style positional encoding
            self.mha_layer = RelPosMHAXL(
                num_heads=nhead,
                embed_dim=d_model,
                dropout=dropout,
                mask_pos_future=causal,
            )
        elif attention_type == "hypermixing":
            self.mha_layer = HyperMixing(
                input_output_dim=d_model,
                hypernet_size=d_ffn,
                tied=False,
                num_heads=nhead,
                fix_tm_hidden_size=False,
            )

        self.convolution_module = ConvolutionModule(
            d_model, kernel_size, bias, activation, dropout, causal=causal
        )

        self.ffn_module1 = nn.Sequential(
            nn.LayerNorm(d_model),
            PositionalwiseFeedForward(
                d_ffn=d_ffn,
                input_size=d_model,
                dropout=dropout,
                activation=activation,
            ),
            nn.Dropout(dropout),
        )

        self.ffn_module2 = nn.Sequential(
            nn.LayerNorm(d_model),
            PositionalwiseFeedForward(
                d_ffn=d_ffn,
                input_size=d_model,
                dropout=dropout,
                activation=activation,
            ),
            nn.Dropout(dropout),
        )

        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(
        self,
        x,
        src_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None,
        pos_embs: torch.Tensor = None,
        dynchunktrain_config: Optional[DynChunkTrainConfig] = None,
    ):
        """
        Arguments
        ----------
        src : torch.Tensor
            The sequence to the encoder layer.
        src_mask : torch.Tensor, optional
            The mask for the src sequence.
        src_key_padding_mask : torch.Tensor, optional
            The mask for the src keys per batch.
        pos_embs: torch.Tensor, torch.nn.Module, optional
            Module or tensor containing the input sequence positional embeddings
        dynchunktrain_config: Optional[DynChunkTrainConfig]
            Dynamic Chunk Training configuration object for streaming,
            specifically involved here to apply Dynamic Chunk Convolution to
            the convolution module.
        """
        conv_mask: Optional[torch.Tensor] = None
        if src_key_padding_mask is not None:
            conv_mask = src_key_padding_mask.unsqueeze(-1)
        # ffn module
        x = x + ConformerConstants.MACARON_FFN_SCALE * self.ffn_module1(x)
        # multi-head attention module
        skip = x
        x = self.norm1(x)

        x, self_attn = self.mha_layer(
            x,
            x,
            x,
            attn_mask=src_mask,
            key_padding_mask=src_key_padding_mask,
            pos_embs=pos_embs,
        )
        x = x + skip
        # convolution module
        x = x + self.convolution_module(
            x, conv_mask, dynchunktrain_config=dynchunktrain_config
        )
        # ffn module
        x = self.norm2(x + ConformerConstants.MACARON_FFN_SCALE * self.ffn_module2(x))
        return x, self_attn

    def forward_streaming(
        self,
        x,
        context: ConformerEncoderLayerStreamingContext,
        pos_embs: torch.Tensor = None,
    ):
        """Conformer layer streaming forward (typically for
        DynamicChunkTraining-trained models), which is to be used at inference
        time. Relies on a mutable context object as initialized by
        `make_streaming_context` that should be used across chunks.
        Invoked by `ConformerEncoder.forward_streaming`.

        Arguments
        ---------
        x : torch.Tensor
            Input tensor for this layer. Batching is supported as long as you
            keep the context consistent.
        context : ConformerEncoderStreamingContext
            Mutable streaming context; the same object should be passed across
            calls.
        pos_embs : torch.Tensor, optional
            Positional embeddings, if used.

        Returns
        -------
        x : torch.Tensor
            Output tensor.
        self_attn : list
            List of self attention values.
        """

        orig_len = x.shape[-2]
        # ffn module
        x = x + ConformerConstants.MACARON_FFN_SCALE * self.ffn_module1(x)

        # TODO: make the approach for MHA left context more efficient.
        # currently, this saves the inputs to the MHA.
        # the naive approach is suboptimal in a few ways, namely that the
        # outputs for this left padding is being re-computed even though we
        # discard them immediately after.

        # left pad `x` with our MHA left context
        if context.mha_left_context is not None:
            x = torch.cat((context.mha_left_context, x), dim=1)

        # compute new MHA left context for the next call to our function
        if context.mha_left_context_size > 0:
            context.mha_left_context = x[
                ..., -context.mha_left_context_size :, :
            ]

        # multi-head attention module
        skip = x
        x = self.norm1(x)

        x, self_attn = self.mha_layer(
            x,
            x,
            x,
            attn_mask=None,
            key_padding_mask=None,
            pos_embs=pos_embs,
        )
        x = x + skip

        # truncate outputs corresponding to the MHA left context (we only care
        # about our chunk's outputs); see above to-do
        x = x[..., -orig_len:, :]

        if context.dcconv_left_context is not None:
            x = torch.cat((context.dcconv_left_context, x), dim=1)

        # compute new DCConv left context for the next call to our function
        context.dcconv_left_context = x[
            ..., -self.convolution_module.padding :, :
        ]

        # convolution module
        x = x + self.convolution_module(x)

        # truncate outputs corresponding to the DCConv left context
        x = x[..., -orig_len:, :]

        # ffn module
        x = self.norm2(x + ConformerConstants.MACARON_FFN_SCALE * self.ffn_module2(x))
        return x, self_attn

    def make_streaming_context(self, mha_left_context_size: int):
        """Creates a blank streaming context for this encoding layer.

        Arguments
        ---------
        mha_left_context_size : int
            How many left frames should be saved and used as left context to the
            current chunk when streaming

        Returns
        -------
        ConformerEncoderLayerStreamingContext
        """
        return ConformerEncoderLayerStreamingContext(
            mha_left_context_size=mha_left_context_size
        )


class ConformerEncoder(nn.Module):
    """Complete Conformer encoder stack for high-performance audio sequence modeling.

    This class implements the full Conformer encoder architecture by stacking multiple
    ConformerEncoderLayer instances. It serves as the primary audio encoder in the
    Mamba-ASR system, combining the global context modeling of transformers with the
    local pattern recognition of convolutional networks for superior ASR performance.

    ARCHITECTURAL OVERVIEW:
    ======================
    The Conformer encoder represents a breakthrough in audio sequence modeling:

    Multi-Layer Architecture:
    - Stack of num_layers ConformerEncoderLayer instances
    - Each layer combines attention, convolution, and feed-forward processing
    - Residual connections and layer normalization throughout
    - Final layer normalization for output stabilization

    Hybrid Processing Design:
    - Global context via multi-head self-attention mechanisms
    - Local pattern recognition via depthwise separable convolution
    - Macaron-style FFN sandwich structure for improved gradient flow
    - Support for multiple attention types and activation functions

    CROSS-FILE INTEGRATION:
    ======================
    Called by:
    - modules/TransformerASR.py: TransformerInterface.__init__() when encoder_module="conformer"
    - modules/Transformer.py: TransformerInterface.encoder creation
    - Training scripts: train_CTC.py and train_S2S.py via ASR pipeline

    Calls to:
    - ConformerEncoderLayer: Individual layer processing for each encoder layer
    - speechbrain.nnet.normalization.LayerNorm: Final output normalization
    - ConformerEncoderStreamingContext: Streaming state management

    STREAMING ARCHITECTURE:
    ======================
    The encoder provides comprehensive streaming inference support:

    Multi-Layer Context Coordination:
    - make_streaming_context(): Creates ConformerEncoderStreamingContext
    - forward_streaming(): Coordinates chunk processing across all layers
    - Context management for both attention and convolution components

    Dynamic Chunk Training Integration:
    - Chunk-based processing with configurable chunk and context sizes
    - Left context preservation across chunk boundaries for all layers
    - Synchronized layer processing with consistent chunking strategies
    - Memory-efficient context management for real-time applications

    LAYER COORDINATION:
    ==================
    The encoder orchestrates sophisticated multi-layer processing:

    Sequential Layer Processing:
    - Each input chunk processed sequentially through all layers
    - Layer outputs feed as inputs to subsequent layers
    - Context updates managed independently per layer
    - Final layer normalization applied to encoder stack output

    Context Synchronization:
    - All layers use consistent chunk size and context window configurations
    - Context buffers managed coordinately to prevent memory fragmentation
    - Device placement synchronized across all layer contexts
    - Error handling coordinated across entire encoder stack

    STATE MANAGEMENT:
    ================
    Key instance variables and their roles:

    - self.layers: ModuleList of ConformerEncoderLayer instances
      * Contains exactly num_layers encoder layers
      * Each layer configured with consistent architectural parameters
      * Sequential processing through layers during forward pass

    - self.norm: Final layer normalization
      * Applied to output of final encoder layer
      * Provides output stabilization for downstream processing
      * Consistent with transformer architecture patterns

    MEMORY OPTIMIZATION:
    ===================
    The encoder implements several memory efficiency strategies:

    Layer-wise Memory Management:
    - Context buffers allocated per layer with coordinated sizing
    - Memory pooling across layers for efficient GPU utilization
    - Automatic garbage collection of intermediate activations
    - Gradient checkpointing support for deep encoder stacks

    Streaming Memory Efficiency:
    - Fixed-size context buffers prevent unbounded memory growth
    - Context sharing and reuse across processing cycles
    - Memory layout optimized for cache efficiency
    - Device memory management for GPU/CPU hybrid processing

    PERFORMANCE CHARACTERISTICS:
    ============================
    The encoder provides excellent performance across multiple dimensions:

    Computational Efficiency:
    - Linear complexity relative to sequence length (vs. quadratic for pure attention)
    - Efficient convolution operations with depthwise separable design
    - Optimized attention mechanisms with multiple implementation options
    - Support for mixed precision training for memory efficiency

    Modeling Accuracy:
    - State-of-the-art results on major ASR benchmarks
    - Superior length generalization compared to pure transformer models
    - Robust performance across diverse acoustic conditions
    - Effective modeling of both local and global audio patterns

    DEVICE COMPATIBILITY:
    ====================
    Comprehensive optimization across compute platforms:

    CUDA Acceleration:
    - Optimized attention kernels with flash attention support
    - cuDNN acceleration for all convolution operations
    - Efficient memory coalescing for multi-layer processing
    - Support for multi-GPU training with data parallelism

    Apple Silicon MPS:
    - MPS-optimized tensor operations throughout encoder stack
    - Unified memory architecture advantages for large context buffers
    - Metal Performance Shaders acceleration for convolution layers
    - Optimized layer normalization for Apple Silicon

    CPU Performance:
    - MKLDNN acceleration where available for convolution and attention
    - Optimized memory layout for CPU cache performance
    - Threading optimization for multi-layer parallel processing
    - Efficient fallback implementations for unsupported operations

    ERROR HANDLING AND ROBUSTNESS:
    ==============================
    Comprehensive error handling throughout the encoder:

    Configuration Validation:
    - Layer count and dimension compatibility verification
    - Attention head and model dimension divisibility checks
    - Kernel size and convolution parameter validation
    - Device placement consistency across all layers

    Runtime Error Recovery:
    - Input shape validation with informative error messages
    - Context state corruption detection and recovery
    - Memory allocation failure handling with automatic retry
    - Gradient flow validation during training

    STREAMING LATENCY OPTIMIZATION:
    ===============================
    Multiple strategies for minimizing end-to-end latency:

    Context Management Optimization:
    - Minimal context update overhead per layer
    - Efficient tensor operations for context preservation
    - Optimized memory layout for streaming workloads
    - Reduced memory allocation churn through context reuse

    Processing Pipeline Optimization:
    - Sequential layer processing with minimal synchronization overhead
    - Optimized tensor operations across layer boundaries
    - Efficient device memory management for GPU acceleration
    - Reduced memory bandwidth requirements through context sharing

    Parameters
    ----------
    num_layers : int
        Number of ConformerEncoderLayer instances in the encoder stack.
        Controls model depth and computational cost.
        Typical values: 12 (base), 18 (large), 24 (extra-large).

    d_model : int
        Hidden dimension size throughout all encoder layers.
        Must be consistent across all transformer components.
        Typical values: 256, 512, 768, 1024 based on model scale.

    d_ffn : int
        Feed-forward network hidden dimension in each layer.
        Typically 4x d_model for optimal parameter efficiency.
        Controls modeling capacity and computational cost per layer.

    nhead : int
        Number of attention heads in each multi-head attention module.
        Must divide d_model evenly (d_model % nhead == 0).
        Typical values: 8 for 512d, 12 for 768d, 16 for 1024d models.

    kernel_size : int, default=31
        Convolution kernel size for ConvolutionModule in each layer.
        Larger kernels capture longer-range dependencies.
        Must be odd for symmetric padding. Typical range: 15-63.

    kdim : int, optional
        Key dimension for attention mechanism in each layer.
        Defaults to d_model if not specified.
        Rarely modified from default in practice.

    vdim : int, optional
        Value dimension for attention mechanism in each layer.
        Defaults to d_model if not specified.
        Rarely modified from default in practice.

    activation : torch.nn.Module, default=Swish
        Activation function for feed-forward networks in all layers.
        
        Swish: Best performance for Conformer, smooth gradients
        GELU: Alternative smooth activation, similar performance
        ReLU: Faster computation, may reduce modeling capacity

    bias : bool, default=True
        Whether to use bias in convolution and linear layers.
        
        True: Standard setting, provides modeling flexibility
        False: Reduces parameters, may improve generalization

    dropout : float, default=0.0
        Dropout probability applied throughout all encoder layers.
        Applied in attention, convolution, and feed-forward components.
        Typical training values: 0.1-0.2 for regularization.

    causal : bool, default=False
        Whether to use causal convolution in all ConvolutionModules.
        
        False: Bidirectional convolution for maximum accuracy
        True: Causal convolution for streaming compatibility

    attention_type : str, default="regularMHA"
        Type of attention mechanism used in all encoder layers.
        
        "regularMHA": Standard multi-head attention
        "RelPosMHAXL": Relative positional multi-head attention
        "hypermixing": HyperMixing attention variant

    Encoder Flow
    ------------
    Input: [batch_size, sequence_length, d_model]
    â Layer 0: ConformerEncoderLayer â [batch_size, sequence_length, d_model]
    â Layer 1: ConformerEncoderLayer â [batch_size, sequence_length, d_model]
    â ...
    â Layer N-1: ConformerEncoderLayer â [batch_size, sequence_length, d_model]
    â Final LayerNorm â [batch_size, sequence_length, d_model]
    â Output: [batch_size, sequence_length, d_model]

    Examples
    --------
    >>> # Standard Conformer encoder
    >>> encoder = ConformerEncoder(
    ...     num_layers=12,
    ...     d_model=512,
    ...     d_ffn=2048,
    ...     nhead=8,
    ...     kernel_size=ConformerConstants.DEFAULT_KERNEL_SIZE,
    ...     attention_type="RelPosMHAXL"
    ... )
    >>> 
    >>> # Process audio features
    >>> audio_features = torch.rand(8, 100, 512)  # [batch, time, features]
    >>> pos_embs = torch.rand(1, 199, 512)  # Relative positional embeddings
    >>> encoded_output, attention_weights = encoder(audio_features, pos_embs=pos_embs)
    >>> encoded_output.shape
    torch.Size([8, 100, 512])
    
    >>> # Streaming inference setup
    >>> from speechbrain.utils.dynamic_chunk_training import DynChunkTrainConfig
    >>> chunk_config = DynChunkTrainConfig(chunk_size=32, left_context_size=8)
    >>> streaming_context = encoder.make_streaming_context(chunk_config)
    >>> 
    >>> # Process streaming chunks
    >>> audio_chunks = [torch.rand(1, 32, 512) for _ in range(5)]
    >>> streaming_outputs = []
    >>> for chunk in audio_chunks:
    ...     output = encoder.forward_streaming(chunk, streaming_context)
    ...     streaming_outputs.append(output)
    >>> torch.cat(streaming_outputs, dim=1).shape  # Concatenated output
    torch.Size([1, 160, 512])

    See Also
    --------
    ConformerEncoderLayer : Individual layer implementation
    ConformerEncoderStreamingContext : Streaming state management
    modules.TransformerASR.TransformerASR : Primary ASR system using Conformer
    speechbrain.nnet.attention.RelPosMHAXL : Relative positional attention
    """

    def __init__(
        self,
        num_layers,
        d_model,
        d_ffn,
        nhead,
        kernel_size=ConformerConstants.DEFAULT_KERNEL_SIZE,
        kdim=None,
        vdim=None,
        activation=Swish,
        bias=True,
        dropout=ConformerConstants.DEFAULT_DROPOUT,
        causal=False,
        attention_type="RelPosMHAXL",
    ):
        super().__init__()

        self.layers = torch.nn.ModuleList(
            [
                ConformerEncoderLayer(
                    d_ffn=d_ffn,
                    nhead=nhead,
                    d_model=d_model,
                    kdim=kdim,
                    vdim=vdim,
                    dropout=dropout,
                    activation=activation,
                    kernel_size=kernel_size,
                    bias=bias,
                    causal=causal,
                    attention_type=attention_type,
                )
                for i in range(num_layers)
            ]
        )
        self.norm = LayerNorm(d_model, eps=ConformerConstants.LAYER_NORM_EPS)
        self.attention_type = attention_type

    def forward(
        self,
        src,
        src_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None,
        pos_embs: Optional[torch.Tensor] = None,
        dynchunktrain_config: Optional[DynChunkTrainConfig] = None,
    ):
        """
        Arguments
        ----------
        src : torch.Tensor
            The sequence to the encoder layer.
        src_mask : torch.Tensor, optional
            The mask for the src sequence.
        src_key_padding_mask : torch.Tensor, optional
            The mask for the src keys per batch.
        pos_embs: torch.Tensor, torch.nn.Module,
            Module or tensor containing the input sequence positional embeddings
            If custom pos_embs are given it needs to have the shape (1, 2*S-1, E)
            where S is the sequence length, and E is the embedding dimension.
        dynchunktrain_config: Optional[DynChunkTrainConfig]
            Dynamic Chunk Training configuration object for streaming,
            specifically involved here to apply Dynamic Chunk Convolution to the
            convolution module.
        """
        if self.attention_type == "RelPosMHAXL":
            if pos_embs is None:
                raise ValueError(
                    "The chosen attention type for the Conformer is RelPosMHAXL. For this attention type, the positional embeddings are mandatory"
                )

        output = src
        attention_lst = []
        for enc_layer in self.layers:
            output, attention = enc_layer(
                output,
                src_mask=src_mask,
                src_key_padding_mask=src_key_padding_mask,
                pos_embs=pos_embs,
                dynchunktrain_config=dynchunktrain_config,
            )
            attention_lst.append(attention)
        output = self.norm(output)

        return output, attention_lst

    def forward_streaming(
        self,
        src: torch.Tensor,
        context: ConformerEncoderStreamingContext,
        pos_embs: Optional[torch.Tensor] = None,
    ):
        """Conformer streaming forward (typically for
        DynamicChunkTraining-trained models), which is to be used at inference
        time. Relies on a mutable context object as initialized by
        `make_streaming_context` that should be used across chunks.

        Arguments
        ---------
        src : torch.Tensor
            Input tensor. Batching is supported as long as you keep the context
            consistent.
        context : ConformerEncoderStreamingContext
            Mutable streaming context; the same object should be passed across
            calls.
        pos_embs : torch.Tensor, optional
            Positional embeddings, if used.

        Returns
        -------
        output : torch.Tensor
            The output of the streaming conformer.
        attention_lst : list
            The attention values.
        """

        if self.attention_type == "RelPosMHAXL":
            if pos_embs is None:
                raise ValueError(
                    "The chosen attention type for the Conformer is RelPosMHAXL. For this attention type, the positional embeddings are mandatory"
                )

        output = src
        attention_lst = []
        for i, enc_layer in enumerate(self.layers):
            output, attention = enc_layer.forward_streaming(
                output, pos_embs=pos_embs, context=context.layers[i]
            )
            attention_lst.append(attention)
        output = self.norm(output)

        return output, attention_lst

    def make_streaming_context(self, dynchunktrain_config: DynChunkTrainConfig):
        """Creates a blank streaming context for the encoder.

        Arguments
        ---------
        dynchunktrain_config: Optional[DynChunkTrainConfig]
            Dynamic Chunk Training configuration object for streaming

        Returns
        -------
        ConformerEncoderStreamingContext
        """
        return ConformerEncoderStreamingContext(
            dynchunktrain_config=dynchunktrain_config,
            layers=[
                layer.make_streaming_context(
                    mha_left_context_size=dynchunktrain_config.left_context_size_frames()
                )
                for layer in self.layers
            ],
        )


class ConformerDecoderLayer(nn.Module):
    """This is an implementation of Conformer encoder layer.

    Arguments
    ---------
    d_model : int
        The expected size of the input embedding.
    d_ffn : int
        Hidden size of self-attention Feed Forward layer.
    nhead : int
        Number of attention heads.
    kernel_size : int, optional
        Kernel size of convolution model.
    kdim : int, optional
        Dimension of the key.
    vdim : int, optional
        Dimension of the value.
    activation : torch.nn.Module, optional
         Activation function used in each Conformer layer.
    bias : bool, optional
        Whether  convolution module.
    dropout : int, optional
        Dropout for the encoder.
    causal : bool, optional
        Whether the convolutions should be causal or not.
    attention_type : str, optional
        type of attention layer, e.g. regularMHA for regular MultiHeadAttention.

    Example
    -------
    >>> import torch
    >>> x = torch.rand((8, 60, 512))
    >>> pos_embs = torch.rand((1, 2*60-1, 512))
    >>> net = ConformerEncoderLayer(d_ffn=512, nhead=8, d_model=512, kernel_size=3)
    >>> output = net(x, pos_embs=pos_embs)
    >>> output[0].shape
    torch.Size([8, 60, 512])
    """

    def __init__(
        self,
        d_model,
        d_ffn,
        nhead,
        kernel_size,
        kdim=None,
        vdim=None,
        activation=Swish,
        bias=True,
        dropout=ConformerConstants.DEFAULT_DROPOUT,
        causal=True,
        attention_type="RelPosMHAXL",
    ):
        super().__init__()

        if not causal:
            warnings.warn(
                "Decoder is not causal, in most applications it should be causal, you have been warned !"
            )

        if attention_type == "regularMHA":
            self.mha_layer = MultiheadAttention(
                nhead=nhead,
                d_model=d_model,
                dropout=dropout,
                kdim=kdim,
                vdim=vdim,
            )
        elif attention_type == "RelPosMHAXL":
            # transformerXL style positional encoding
            self.mha_layer = RelPosMHAXL(
                num_heads=nhead,
                embed_dim=d_model,
                dropout=dropout,
                mask_pos_future=causal,
            )

        self.convolution_module = ConvolutionModule(
            d_model, kernel_size, bias, activation, dropout, causal=causal
        )

        self.ffn_module1 = nn.Sequential(
            nn.LayerNorm(d_model),
            PositionalwiseFeedForward(
                d_ffn=d_ffn,
                input_size=d_model,
                dropout=dropout,
                activation=activation,
            ),
            nn.Dropout(dropout),
        )

        self.ffn_module2 = nn.Sequential(
            nn.LayerNorm(d_model),
            PositionalwiseFeedForward(
                d_ffn=d_ffn,
                input_size=d_model,
                dropout=dropout,
                activation=activation,
            ),
            nn.Dropout(dropout),
        )

        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(
        self,
        tgt,
        memory,
        tgt_mask=None,
        memory_mask=None,
        tgt_key_padding_mask=None,
        memory_key_padding_mask=None,
        pos_embs_tgt=None,
        pos_embs_src=None,
    ):
        """
        Arguments
        ---------
        tgt: torch.Tensor
            The sequence to the decoder layer.
        memory: torch.Tensor
            The sequence from the last layer of the encoder.
        tgt_mask: torch.Tensor, optional, optional
            The mask for the tgt sequence.
        memory_mask: torch.Tensor, optional
            The mask for the memory sequence.
        tgt_key_padding_mask: torch.Tensor, optional
            The mask for the tgt keys per batch.
        memory_key_padding_mask: torch.Tensor, optional
            The mask for the memory keys per batch.
        pos_embs_tgt: torch.Tensor, torch.nn.Module, optional
            Module or tensor containing the target sequence positional embeddings for each attention layer.
        pos_embs_src: torch.Tensor, torch.nn.Module, optional
            Module or tensor containing the source sequence positional embeddings for each attention layer.

        Returns
        -------
        x: torch.Tensor
            The output tensor
        self_attn : torch.Tensor
        self_attn : torch.Tensor
            The self attention tensor
        """
        # ffn module
        tgt = tgt + ConformerConstants.MACARON_FFN_SCALE * self.ffn_module1(tgt)
        # multi-head attention module
        skip = tgt
        x = self.norm1(tgt)
        x, self_attn = self.mha_layer(
            x,
            memory,
            memory,
            attn_mask=memory_mask,
            key_padding_mask=memory_key_padding_mask,
            pos_embs=pos_embs_src,
        )
        x = x + skip
        # convolution module
        x = x + self.convolution_module(x)
        # ffn module
        x = self.norm2(x + ConformerConstants.MACARON_FFN_SCALE * self.ffn_module2(x))
        return x, self_attn, self_attn


class ConformerDecoder(nn.Module):
    """This class implements the Transformer decoder.

    Arguments
    ---------
    num_layers: int
        Number of layers.
    nhead: int
        Number of attention heads.
    d_ffn: int
        Hidden size of self-attention Feed Forward layer.
    d_model: int
        Embedding dimension size.
    kdim: int, optional
        Dimension for key.
    vdim: int, optional
        Dimension for value.
    dropout: float, optional
        Dropout rate.
    activation: torch.nn.Module, optional
        Activation function used after non-bottleneck conv layer.
    kernel_size : int, optional
        Kernel size of convolutional layer.
    bias : bool, optional
        Whether  convolution module.
    causal: bool, optional
        Whether the convolutions should be causal or not.
    attention_type: str, optional
        type of attention layer, e.g. regularMHA for regular MultiHeadAttention.


    Example
    -------
    >>> src = torch.rand((8, 60, 512))
    >>> tgt = torch.rand((8, 60, 512))
    >>> net = ConformerDecoder(1, 8, 1024, 512, attention_type="regularMHA")
    >>> output, _, _ = net(tgt, src)
    >>> output.shape
    torch.Size([8, 60, 512])
    """

    def __init__(
        self,
        num_layers,
        nhead,
        d_ffn,
        d_model,
        kdim=None,
        vdim=None,
        dropout=ConformerConstants.DEFAULT_DROPOUT,
        activation=Swish,
        kernel_size=3,
        bias=True,
        causal=True,
        attention_type="RelPosMHAXL",
    ):
        super().__init__()
        self.layers = torch.nn.ModuleList(
            [
                ConformerDecoderLayer(
                    d_ffn=d_ffn,
                    nhead=nhead,
                    d_model=d_model,
                    kdim=kdim,
                    vdim=vdim,
                    dropout=dropout,
                    activation=activation,
                    kernel_size=kernel_size,
                    bias=bias,
                    causal=causal,
                    attention_type=attention_type,
                )
                for _ in range(num_layers)
            ]
        )
        self.norm = sb.nnet.normalization.LayerNorm(d_model, eps=ConformerConstants.LAYER_NORM_EPS)

    def forward(
        self,
        tgt,
        memory,
        tgt_mask=None,
        memory_mask=None,
        tgt_key_padding_mask=None,
        memory_key_padding_mask=None,
        pos_embs_tgt=None,
        pos_embs_src=None,
    ):
        """
        Arguments
        ---------
        tgt: torch.Tensor
            The sequence to the decoder layer.
        memory: torch.Tensor
            The sequence from the last layer of the encoder.
        tgt_mask: torch.Tensor, optional, optional
            The mask for the tgt sequence.
        memory_mask: torch.Tensor, optional
            The mask for the memory sequence.
        tgt_key_padding_mask : torch.Tensor, optional
            The mask for the tgt keys per batch.
        memory_key_padding_mask : torch.Tensor, optional
            The mask for the memory keys per batch.
        pos_embs_tgt: torch.Tensor, torch.nn.Module, optional
            Module or tensor containing the target sequence positional embeddings for each attention layer.
        pos_embs_src: torch.Tensor, torch.nn.Module, optional
            Module or tensor containing the source sequence positional embeddings for each attention layer.

        Returns
        -------
        output: torch.Tensor
            Conformer decoder output.
        self_attns : list
            Location of self attentions.
        multihead_attns : list
            Location of multihead attentions.
        """
        output = tgt
        self_attns, multihead_attns = [], []
        for dec_layer in self.layers:
            output, self_attn, multihead_attn = dec_layer(
                output,
                memory,
                tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=memory_key_padding_mask,
                pos_embs_tgt=pos_embs_tgt,
                pos_embs_src=pos_embs_src,
            )
            self_attns.append(self_attn)
            multihead_attns.append(multihead_attn)
        output = self.norm(output)

        return output, self_attns, multihead_attns
